@inproceedings{albergoUnderstandingXacroMisunderstandings2022,
  title = {Understanding {{Xacro Misunderstandings}}},
  booktitle = {2022 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Albergo, Nicholas and Rathi, Vivek and Ore, John-Paul},
  date = {2022-05},
  pages = {6247--6252},
  doi = {10.1109/ICRA46639.2022.9812349},
  url = {https://ieeexplore.ieee.org/document/9812349/?arnumber=9812349},
  urldate = {2024-12-05},
  abstract = {The Xacro XML macro language can be used to augment the Universal Robot Description Format (URDF) and is part of a critical toolchain from geometric representations to simulation, visualization, and system execution. However, mem-bers of the robotics community, especially newcomers, struggle to troubleshoot and understand the interplay between systems and the Xacro preprocessing pipeline. To better understand how system developers struggle with Xacros, we manually examine 712 Xacro-related questions from the question and answer site answers.ros.org and find Xacro misunderstandings fit into eight key categories using a systematic, qualitative approach called Open Coding. By examining the 'tags' applied to questions, we further find that Xacro problems manifest in a befuddlingly broad set of contexts. This hinders onboarding and complicates system developers' understanding of representations and tools in the Robot Operating System. We aim to provide an empirical grounding that identifies and prioritizes impediments to users of open robotics systems, so that tool designers, teachers, and robotics practitioners can devise ways of improving robot software tooling and education.},
  eventtitle = {2022 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  keywords = {Education,Grounding,Meth-ods and Tools for Robot System Design,Operating systems,Pipelines,Software Tools for Robot Programming,Software-Hardware Integration for Robot Systems,Systematics,Visualization,XML},
  file = {C\:\\Users\\noiri\\Zotero\\storage\\DTGUH3TA\\Albergo et al. - 2022 - Understanding Xacro Misunderstandings.pdf;C\:\\Users\\noiri\\Zotero\\storage\\672PVYYA\\9812349.html}
}

@article{aldakheelDetectionIdentificationPlant2024,
  title = {Detection and Identification of Plant Leaf Diseases Using {{YOLOv4}}},
  author = {Aldakheel, Eman Abdullah and Zakariah, Mohammed and Alabdalall, Amira H.},
  date = {2024},
  journaltitle = {Frontiers in Plant Science},
  shortjournal = {Front Plant Sci},
  volume = {15},
  eprint = {38711603},
  eprinttype = {pubmed},
  pages = {1355941},
  issn = {1664-462X},
  doi = {10.3389/fpls.2024.1355941},
  abstract = {Detecting plant leaf diseases accurately and promptly is essential for reducing economic consequences and maximizing crop yield. However, farmers' dependence on conventional manual techniques presents a difficulty in accurately pinpointing particular diseases. This research investigates the utilization of the YOLOv4 algorithm for detecting and identifying plant leaf diseases. This study uses the comprehensive Plant Village Dataset, which includes over fifty thousand photos of healthy and diseased plant leaves from fourteen different species, to develop advanced disease prediction systems in agriculture. Data augmentation techniques including histogram equalization and horizontal flip were used to improve the dataset and strengthen the model's resilience. A comprehensive assessment of the YOLOv4 algorithm was conducted, which involved comparing its performance with established target identification methods including Densenet, Alexanet, and neural networks. When YOLOv4 was used on the Plant Village dataset, it achieved an impressive accuracy of 99.99\%. The evaluation criteria, including accuracy, precision, recall, and f1-score, consistently showed high performance with a value of 0.99, confirming the effectiveness of the proposed methodology. This study's results demonstrate substantial advancements in plant disease detection and underscore the capabilities of YOLOv4 as a sophisticated tool for accurate disease prediction. These developments have significant significance for everyone involved in agriculture, researchers, and farmers, providing improved capacities for disease control and crop protection.},
  langid = {english},
  pmcid = {PMC11070553},
  keywords = {darknet,deep-learning,leaf disease detection,object detection,plant leaf disease,YOLO v4},
  file = {C:\Users\noiri\Zotero\storage\2V95WAYJ\Aldakheel et al. - 2024 - Detection and identification of plant leaf diseases using YOLOv4.pdf}
}

@software{alejhernandezcorderoandrohernandezcorderoRos_gzRos_gz_bridgeTest,
  title = {Ros\_gz/Ros\_gz\_bridge/Test/Config/Full.Yaml at Ros2 · Gazebosim/Ros\_gz},
  author = {AlejHernández Corderoandro Hernández Cordero, Alejandro},
  url = {https://github.com/gazebosim/ros_gz/blob/ros2/ros_gz_bridge/test/config/full.yaml},
  urldate = {2024-12-05},
  abstract = {Integration between ROS (1 and 2) and Gazebo simulation - gazebosim/ros\_gz},
  version = {Alejandro Hernández Cordero},
  file = {C:\Users\noiri\Zotero\storage\DBGX4XWC\ros_gz_bridge.html}
}

@online{AnswerDisadvantageUsing2021,
  title = {Answer to "{{Disadvantage}} of Using --Allow-Releaseinfo-Change for Apt-Get Update"},
  date = {2021-08-30},
  url = {https://stackoverflow.com/a/68990686/27774705},
  urldate = {2025-04-03},
  organization = {Stack Overflow},
  file = {C:\Users\noiri\Zotero\storage\Y7R9NU5L\disadvantage-of-using-allow-releaseinfo-change-for-apt-get-update.html}
}

@article{antonettiAdvancedEnvironmentalStatistics,
  title = {Advanced {{Environmental Statistics}} – {{HS}} 2024, {{Lesson}} 06 – {{Homework Assignment}}},
  author = {Antonetti, Manuel},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\T3JLK6EV\Antonetti - Advanced Environmental Statistics – HS 2024, Lesson 06 – Homework Assignment.pdf}
}

@online{Aptget8AptDebian,
  title = {Apt-Get(8) — Apt — {{Debian}} Unstable — {{Debian Manpages}}},
  url = {https://manpages.debian.org/unstable/apt/apt-get.8.en.html},
  urldate = {2025-04-03},
  file = {C:\Users\noiri\Zotero\storage\KMRHSA6T\apt-get.8.en.html}
}

@online{Aptsecure8AptDebian,
  title = {Apt-Secure(8) — Apt — {{Debian}} Unstable — {{Debian Manpages}}},
  url = {https://manpages.debian.org/unstable/apt/apt-secure.8.en.html},
  urldate = {2025-04-03},
  file = {C:\Users\noiri\Zotero\storage\KFF7WE5T\apt-secure.8.en.html}
}

@software{asadaTasada038Fuzzy_controller_ros2024,
  title = {Tasada038/Fuzzy\_controller\_ros},
  author = {Asada, Takumi},
  date = {2024-11-19T11:35:19Z},
  origdate = {2022-12-21T04:12:01Z},
  url = {https://github.com/tasada038/fuzzy_controller_ros},
  urldate = {2025-03-03},
  abstract = {ROS2 package for fuzzy control.},
  keywords = {cpp,fuzzy-logic,ros2}
}

@online{automaticaddisonCompleteGuideDocker2024,
  title = {The {{Complete Guide}} to {{Docker}} for {{ROS}} 2 {{Jazzy Projects}}},
  author = {{automaticaddison}},
  date = {2024-12-24},
  url = {https://automaticaddison.com/the-complete-guide-to-docker-for-ros-2-jazzy-projects/},
  urldate = {2025-03-05},
  langid = {american},
  file = {C:\Users\noiri\Zotero\storage\YINPW6EB\the-complete-guide-to-docker-for-ros-2-jazzy-projects.html}
}

@online{automaticaddisonHowInstallROS2024,
  title = {How to {{Install ROS}} 2 {{Navigation}} ({{Nav2}}) – {{ROS}} 2 {{Jazzy}}},
  author = {{automaticaddison}},
  date = {2024-11-27},
  url = {https://automaticaddison.com/how-to-install-ros-2-navigation-nav2-ros-2-jazzy/},
  urldate = {2025-04-09},
  langid = {american},
  file = {C:\Users\noiri\Zotero\storage\VEXJRE8J\how-to-install-ros-2-navigation-nav2-ros-2-jazzy.html}
}

@online{automaticaddisonUsefulWorldFiles2021,
  title = {Useful {{World Files}} for {{Gazebo}} and {{ROS}} 2 {{Simulations}}},
  author = {{automaticaddison}},
  date = {2021-09-25},
  url = {https://automaticaddison.com/useful-world-files-for-gazebo-and-ros-2-simulations/},
  urldate = {2025-03-03},
  langid = {american},
  file = {C:\Users\noiri\Zotero\storage\TPE2NZUV\useful-world-files-for-gazebo-and-ros-2-simulations.html}
}

@article{bakAgriculturalRoboticPlatform2004,
  title = {Agricultural {{Robotic Platform}} with {{Four Wheel Steering}} for {{Weed Detection}}},
  author = {Bak, Thomas and Jakobsen, Hans},
  date = {2004-02},
  journaltitle = {Biosystems Engineering},
  shortjournal = {Biosystems Engineering},
  volume = {87},
  number = {2},
  pages = {125--136},
  issn = {15375110},
  doi = {10.1016/j.biosystemseng.2003.10.009},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S153751100300196X},
  urldate = {2025-04-21},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\N7F8MTJH\Bak und Jakobsen - 2004 - Agricultural Robotic Platform with Four Wheel Steering for Weed Detection.pdf}
}

@article{boettigerIntroductionDockerReproducible2015,
  title = {An Introduction to {{Docker}} for Reproducible Research},
  author = {Boettiger, Carl},
  date = {2015-01-20},
  journaltitle = {ACM SIGOPS Operating Systems Review},
  shortjournal = {SIGOPS Oper. Syst. Rev.},
  volume = {49},
  number = {1},
  pages = {71--79},
  issn = {0163-5980},
  doi = {10.1145/2723872.2723882},
  url = {https://dl.acm.org/doi/10.1145/2723872.2723882},
  urldate = {2025-03-23},
  abstract = {As computational work becomes more and more integral to many aspects of scientific research, computational reproducibility has become an issue of increasing importance to computer systems researchers and domain scientists alike. Though computational reproducibility seems more straight forward than replicating physical experiments, the complex and rapidly changing nature of computer environments makes being able to reproduce and extend such work a serious challenge. In this paper, I explore common reasons that code developed for one research project cannot be successfully executed or extended by subsequent researchers. I review current approaches to these issues, including virtual machines and workflow systems, and their limitations. I then examine how the popular emerging technology Docker combines several areas from systems research - such as operating system virtualization, cross-platform portability, modular re-usable elements, versioning, and a 'DevOps' philosophy, to address these challenges. I illustrate this with several examples of Docker use with a focus on the R statistical environment.},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\IM47WXAG\Boettiger - 2015 - An introduction to Docker for reproducible research.pdf}
}

@video{briandouglasPIDControlBrief2012,
  entrysubtype = {video},
  title = {{{PID Control}} - {{A}} Brief Introduction},
  editor = {{Brian Douglas}},
  editortype = {director},
  date = {2012-12-14},
  url = {https://www.youtube.com/watch?v=UR0hOmjaHp0},
  urldate = {2025-03-03}
}

@video{briandouglasSimpleExamplesPID2012,
  entrysubtype = {video},
  title = {Simple {{Examples}} of {{PID Control}}},
  editor = {{Brian Douglas}},
  editortype = {director},
  date = {2012-12-22},
  url = {https://www.youtube.com/watch?v=XfAt6hNV8XM},
  urldate = {2025-03-03}
}

@article{cardemaOptimalPathPlanning2004,
  title = {Optimal Path Planning of Mobile Robots for Sample Collection},
  author = {Cardema, J. C. and Wang, P. K. C. and Rodriguez, G.},
  date = {2004},
  journaltitle = {Journal of Robotic Systems},
  volume = {21},
  number = {10},
  pages = {559--580},
  issn = {1097-4563},
  doi = {10.1002/rob.20036},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.20036},
  urldate = {2025-04-28},
  abstract = {In this paper, the problem of path planning for sample collection using single or multiple mobile robots such as Mars rovers is formulated as a mathematical optimization problem involving a performance metric based on the scientific values of the collected rock and soil samples. The posed optimization problem is NP-hard and more complex than the well-known Traveling Salesman Problem. Algorithms are proposed for obtaining near-optimal solutions for both single and multiple robots. Their application is illustrated using real Mars surface data. The dependence of the optimal performance on the number of mobile robots is studied numerically. © 2004 Wiley Periodicals, Inc.},
  langid = {english},
  file = {C\:\\Users\\noiri\\Zotero\\storage\\Y9LRB249\\Cardema et al. - 2004 - Optimal path planning of mobile robots for sample collection.pdf;C\:\\Users\\noiri\\Zotero\\storage\\LVHC3QT7\\rob.html}
}

@online{chenAutoBagLearningOpen2023,
  title = {{{AutoBag}}: {{Learning}} to {{Open Plastic Bags}} and {{Insert Objects}}},
  shorttitle = {{{AutoBag}}},
  author = {Chen, Lawrence Yunliang and Shi, Baiyu and Seita, Daniel and Cheng, Richard and Kollar, Thomas and Held, David and Goldberg, Ken},
  date = {2023-03-19},
  eprint = {2210.17217},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.17217},
  url = {http://arxiv.org/abs/2210.17217},
  urldate = {2025-04-28},
  abstract = {Thin plastic bags are ubiquitous in retail stores, healthcare, food handling, recycling, homes, and school lunchrooms. They are challenging both for perception (due to specularities and occlusions) and for manipulation (due to the dynamics of their 3D deformable structure). We formulate the task of "bagging:" manipulating common plastic shopping bags with two handles from an unstructured initial state to an open state where at least one solid object can be inserted into the bag and lifted for transport. We propose a self-supervised learning framework where a dual-arm robot learns to recognize the handles and rim of plastic bags using UV-fluorescent markings; at execution time, the robot does not use UV markings or UV light. We propose the AutoBag algorithm, where the robot uses the learned perception model to open a plastic bag through iterative manipulation. We present novel metrics to evaluate the quality of a bag state and new motion primitives for reorienting and opening bags based on visual observations. In physical experiments, a YuMi robot using AutoBag is able to open bags and achieve a success rate of 16/30 for inserting at least one item across a variety of initial bag configurations. Supplementary material is available at https://sites.google.com/view/autobag.},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics},
  file = {C\:\\Users\\noiri\\Zotero\\storage\\DUI3EDY2\\Chen et al. - 2023 - AutoBag Learning to Open Plastic Bags and Insert Objects.pdf;C\:\\Users\\noiri\\Zotero\\storage\\X99P45WS\\2210.html}
}

@article{choPlantGrowthInformation2023,
  title = {Plant Growth Information Measurement Based on Object Detection and Image Fusion Using a Smart Farm Robot},
  author = {Cho, Songhee and Kim, Taehyeong and Jung, Dae-Hyun and Park, Soo Hyun and Na, Yunseong and Ihn, Yong Seok and Kim, KangGeon},
  date = {2023-04-01},
  journaltitle = {Computers and Electronics in Agriculture},
  shortjournal = {Computers and Electronics in Agriculture},
  volume = {207},
  pages = {107703},
  issn = {0168-1699},
  doi = {10.1016/j.compag.2023.107703},
  url = {https://www.sciencedirect.com/science/article/pii/S0168169923000911},
  urldate = {2025-04-16},
  abstract = {Traditionally, vegetable and fruit production has relied on empirical and ambiguous decisions made by human farmers. To overcome this uncertainty in agriculture, smart farm robots have been widely studied in recent years. However, measuring growth information with robots remains a challenge because of the similarity in the appearance of the target plant and those around it. In this study, we propose a smart farm robot that accurately measures the growth information of a target plant based on object detection, image fusion, and data augmentation with fused images. The proposed smart farm robot uses an end-to-end real-time deep learning-based object detector that shows state-of-the-art performances. To distinguish the target plant from other plants with a higher accuracy and improved robustness than those of existing methods, we exploited image fusion using both RGB and depth images. In particular, the data augmentation, based on the fused RGB, and depth information, contributes to the precise measurement of growth information from smart farms, regardless of the high density of vegetables and fruits in these farms. We propose and evaluate a real-time measurement system to obtain precise target-plant growth information in precision agriculture. The code and models are publicly available on Github: https://github.com/kistvision/Plant\_growth\_measurement.},
  keywords = {Data augmentation,Deep learning in agriculture,Image fusion,Plant growth information measurement,Smart farm robot},
  file = {C:\Users\noiri\Zotero\storage\PVHWQSYQ\S0168169923000911.html}
}

@online{chowDisadvantageUsingAllowreleaseinfochange2021,
  type = {Forum post},
  title = {Disadvantage of Using --Allow-Releaseinfo-Change for Apt-Get Update},
  author = {Chow, Jonathan},
  date = {2021-08-19},
  url = {https://stackoverflow.com/q/68849201/27774705},
  urldate = {2025-04-03},
  organization = {Stack Overflow},
  file = {C:\Users\noiri\Zotero\storage\BNVWMKIJ\disadvantage-of-using-allow-releaseinfo-change-for-apt-get-update.html}
}

@online{cleggBuildingMultiArchitectureContainers2022,
  title = {Building {{Multi-Architecture Containers}} for {{OCI}} with {{Docker}}},
  author = {Clegg, Tim},
  date = {2022-05-23T11:02:29},
  url = {https://medium.com/@timclegg/building-multi-architecture-containers-for-oci-with-docker-59cea3b5a8c4},
  urldate = {2025-03-05},
  abstract = {How to build multi-architecture (arm64 and amd64) containers for Oracle Cloud Infrastructure with Docker Desktop and Docker Engine.},
  langid = {english},
  organization = {Medium},
  file = {C:\Users\noiri\Zotero\storage\7PPY6E4R\building-multi-architecture-containers-for-oci-with-docker-59cea3b5a8c4.html}
}

@article{coldingIncrementalDemiseUrban2020,
  title = {The {{Incremental Demise}} of {{Urban Green Spaces}}},
  author = {Colding, Johan and Gren, Åsa and Barthel, Stephan},
  date = {2020-05},
  journaltitle = {Land},
  volume = {9},
  number = {5},
  pages = {162},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2073-445X},
  doi = {10.3390/land9050162},
  url = {https://www.mdpi.com/2073-445X/9/5/162},
  urldate = {2025-03-17},
  abstract = {More precise explanations are needed to better understand why public green spaces are diminishing in cities, leading to the loss of ecosystem services that humans receive from natural systems. This paper is devoted to the incremental change of green spaces—a fate that is largely undetectable by urban residents. The paper elucidates a set of drivers resulting in the subtle loss of urban green spaces and elaborates on the consequences of this for resilience planning of ecosystem services. Incremental changes of greenspace trigger baseline shifts, where each generation of humans tends to take the current condition of an ecosystem as the normal state, disregarding its previous states. Even well-intended political land-use decisions, such as current privatization schemes, can cumulatively result in undesirable societal outcomes, leading to a gradual loss of opportunities for nature experience. Alfred E. Kahn referred to such decision making as ‘the tyranny of small decisions.’ This is mirrored in urban planning as problems that are dealt with in an ad hoc manner with no officially formulated vision for long-term spatial planning. Urban common property systems could provide interim solutions for local governments to survive periods of fiscal shortfalls. Transfer of proprietor rights to civil society groups can enhance the resilience of ecosystem services in cities.},
  issue = {5},
  langid = {english},
  keywords = {baseline shifts,ecosystem services,incremental greenspace loss,privatization,property rights,resilience planning,the tyranny of small decisions,urban densification,urban greenspace,urban nature connection},
  file = {C:\Users\noiri\Zotero\storage\G9RFEKM3\Colding et al. - 2020 - The Incremental Demise of Urban Green Spaces.pdf}
}

@inreference{ControlTheory2025,
  title = {Control Theory},
  booktitle = {Wikipedia},
  date = {2025-03-17T01:00:08Z},
  url = {https://en.wikipedia.org/w/index.php?title=Control_theory&oldid=1280889989},
  urldate = {2025-04-12},
  abstract = {Control theory is a field of control engineering and applied mathematics that deals with the control of dynamical systems in engineered processes and machines. The objective is to develop a model or algorithm governing the application of system inputs to drive the system to a desired state, while minimizing any delay, overshoot, or steady-state error and ensuring a level of control stability; often with the aim to achieve a degree of optimality. To do this, a controller with the requisite corrective behavior is required. This controller monitors the controlled process variable (PV), and compares it with the reference or set point (SP). The difference between actual and desired value of the process variable, called the error signal, or SP-PV error, is applied as feedback to generate a control action to bring the controlled process variable to the same value as the set point. Other aspects which are also studied are  controllability and observability.  Control theory is used in control system engineering to design automation  that have revolutionized manufacturing, aircraft, communications and other industries, and created new fields such as robotics.   Extensive use is usually made of a diagrammatic style known as the block diagram. In it the transfer function, also known as the system function or network function, is a mathematical model of the relation between the input and output based on the differential equations describing the system. Control theory dates from the 19th century, when the theoretical basis for the operation of governors was first described by James Clerk Maxwell.  Control theory was further advanced by Edward Routh in 1874, Charles Sturm and in 1895, Adolf Hurwitz, who all contributed to the establishment of control stability criteria; and from 1922 onwards, the development of PID control theory by Nicolas Minorsky. Although a major application of mathematical control theory is in control systems engineering, which deals with the design of process control systems for industry, other applications range far beyond this. As the general theory of feedback systems, control theory is useful wherever feedback occurs - thus control theory also has applications in life sciences, computer engineering, sociology and operations research.},
  langid = {english},
  annotation = {Page Version ID: 1280889989},
  file = {C:\Users\noiri\Zotero\storage\ZETE2NGK\Control_theory.html}
}

@inproceedings{damianUsingFullyConvolutional2019,
  title = {Using {{Fully Convolutional Networks}} for {{Rumex Obtusifolius Segmentation}}, a {{Preliminary Report}}},
  booktitle = {2019 {{International Symposium ELMAR}}},
  author = {Damian, Schori and Thomas, Anken and Dejan, Šeatović},
  date = {2019-09},
  pages = {119--122},
  issn = {1334-2630},
  doi = {10.1109/ELMAR.2019.8918914},
  url = {https://ieeexplore.ieee.org/document/8918914/?arnumber=8918914},
  urldate = {2024-07-23},
  abstract = {Image segmentation of specific plants is an important task in precision farming. Several influences such as changing light, varying arrangement of leaves and similarly looking plants are challenging. We present a solution for segmenting individual Rumex obtusifolius plants out of complicated natural scenes in grassland from 2D images. We are making use of a fully convolutional deep neural network (FCN) trained with hand labeled images. The proposed segmentation scheme is validated with images taken under outdoor conditions. The overall masks segmentation rate is 84.8\% measured by the dice coefficient. Approximately half of the experiments show segmentation rates of individual plants higher than 88\%. The developed solution is therefore a robust method to segment Rumex obtusifolius plants under real-world conditions in short time.},
  eventtitle = {2019 {{International Symposium ELMAR}}},
  keywords = {Computer Vision,Deep Learning,Image segmentation,Image Segmentation,Object segmentation,Real-time systems,Robots,Robustness,Rumex obtusifolius,Three-dimensional displays,Training},
  file = {C\:\\Users\\noiri\\Zotero\\storage\\N5NJURKD\\Damian et al. - 2019 - Using Fully Convolutional Networks for Rumex Obtus.pdf;C\:\\Users\\noiri\\Zotero\\storage\\3EX4GQ5J\\8918914.html}
}

@article{decroonInsectinspiredAIAutonomous2022,
  title = {Insect-Inspired {{AI}} for Autonomous Robots},
  author = {De Croon, G. C. H. E. and Dupeyroux, J. J. G. and Fuller, S. B. and Marshall, J. A. R.},
  date = {2022-06-29},
  journaltitle = {Science Robotics},
  shortjournal = {Sci. Robot.},
  volume = {7},
  number = {67},
  pages = {eabl6334},
  issn = {2470-9476},
  doi = {10.1126/scirobotics.abl6334},
  url = {https://www.science.org/doi/10.1126/scirobotics.abl6334},
  urldate = {2025-02-25},
  abstract = {Autonomous robots are expected to perform a wide range of sophisticated tasks in complex, unknown environments. However, available onboard computing capabilities and algorithms represent a considerable obstacle to reaching higher levels of autonomy, especially as robots get smaller and the end of Moore’s law approaches. Here, we argue that inspiration from insect intelligence is a promising alternative to classic methods in robotics for the artificial intelligence (AI) needed for the autonomy of small, mobile robots. The advantage of insect intelligence stems from its resource efficiency (or parsimony) especially in terms of power and mass. First, we discuss the main aspects of insect intelligence underlying this parsimony: embodiment, sensory-motor coordination, and swarming. Then, we take stock of where insect-inspired AI stands as an alternative to other approaches to important robotic tasks such as navigation and identify open challenges on the road to its more widespread adoption. Last, we reflect on the types of processors that are suitable for implementing insect-inspired AI, from more traditional ones such as microcontrollers and field-programmable gate arrays to unconventional neuromorphic processors. We argue that even for neuromorphic processors, one should not simply apply existing AI algorithms but exploit insights from natural insect intelligence to get maximally efficient AI for robot autonomy.           ,              We discuss insect-inspired artificial intelligence as the key to autonomous robots with extremely limited computing power.},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\VW6RH829\De Croon et al. - 2022 - Insect-inspired AI for autonomous robots.pdf}
}

@online{dejanHowControlStepper2015,
  title = {How {{To Control}} a {{Stepper Motor}} with {{A4988 Driver}} and {{Arduino}}},
  author = {Dejan},
  date = {2015-08-16T11:01:33+00:00},
  url = {https://howtomechatronics.com/tutorials/arduino/how-to-control-stepper-motor-with-a4988-driver-and-arduino/},
  urldate = {2024-08-23},
  abstract = {In this Arduino Tutorial we will learn how to control a Stepper Motor using the A4988 Stepper Driver. The A4988 is a microstepping driver for controlling bipolar stepper motors which has built-in translator for easy operation. This means that we can control the stepper motor with...},
  langid = {american},
  organization = {How To Mechatronics},
  file = {C:\Users\noiri\Zotero\storage\RIS89YKB\how-to-control-stepper-motor-with-a4988-driver-and-arduino.html}
}

@online{dirkUniversalBuildTool,
  title = {A Universal Build Tool},
  author = {Dirk, Thomas},
  url = {https://design.ros2.org/articles/build_tool.html},
  urldate = {2024-11-19},
  file = {C:\Users\noiri\Zotero\storage\DY2IAG4C\build_tool.html}
}

@software{dockerHowComposeWorks,
  title = {How {{Compose}} Works},
  author = {Docker},
  url = {https://docs.docker.com/compose/intro/compose-application-model/}
}

@software{dockerRosOfficialImage,
  title = {Ros - {{Official Image}} | {{Docker Hub}}},
  author = {Docker},
  url = {https://hub.docker.com/_/ros},
  urldate = {2024-11-18},
  abstract = {The Robot Operating System (ROS) is an open source project for building robot applications.},
  file = {C:\Users\noiri\Zotero\storage\Q25PV4QU\ros.html}
}

@online{DockerSwarmVs,
  title = {Docker {{Swarm}} vs. {{Kubernetes}} - {{Key Differences Explained}}},
  url = {https://spacelift.io/blog/[slug]},
  urldate = {2025-03-25},
  abstract = {Discover the similarities and differences between Docker Swarm and Kubernetes and see which is better for your use case.},
  langid = {english},
  organization = {Spacelift},
  file = {C:\Users\noiri\Zotero\storage\XTC7ZFUV\docker-swarm-vs-kubernetes.html}
}

@software{dockerWhatDocker,
  title = {What Is {{Docker}}?},
  author = {Docker},
  url = {https://docs.docker.com/get-started/docker-overview/}
}

@thesis{dollingerKameragefuehrtePositionierungUnd2014,
  type = {Thesis},
  title = {Kamerageführte Positionierung und Greifbewegung eines Roboterarms},
  author = {Dollinger, Moritz},
  date = {2014-11-13},
  institution = {Hochschule für angewandte Wissenschaften Hamburg},
  url = {https://reposit.haw-hamburg.de/handle/20.500.12738/6774},
  urldate = {2024-03-06},
  abstract = {Ziel dieser Arbeit ist die bildbasierte Detektion und Rekonstruktion der Positionen von  Objekten im 3D-Raum für eine visuelle Steuerung eines Knickarmroboters. Dieser  soll die detektierten Gegenstände anfahren, greifen und der Größe nach sortieren.  Die Aufnahmen des Arbeitsraums des Roboterarms sollen aus verschiedenen  Perspektiven mit Hilfe zweier Kameras gewonnen werden. Durch einen einmaligen  Kalibriervorgang sollen die Positionen und die internen Parameter der verwendeten  Kameras zunächst bestimmt werden. Mit diesen Parametern können dann aus den  2D-Pixelkoordinaten der Objekte und des Greifers des Roboterarms die jeweiligen  Positionen im 3D-Raum rekonstruiert werden. Anschließend soll eine Bahnplanung  für den Greifer unter Verwendung der inversen Kinematik erfolgen, um die einzelnen  Objekte zu bewegen.},
  langid = {ngerman},
  annotation = {Accepted: 2020-09-29T12:46:13Z},
  file = {C:\Users\noiri\Zotero\storage\4NDUGRAC\Dollinger - 2014 - Kamerageführte Positionierung und Greifbewegung ei.pdf}
}

@article{douchaProductivityCO2O22006,
  title = {Productivity, {{CO2}}/{{O2}} Exchange and Hydraulics in Outdoor Open High Density Microalgal ({{Chlorella}} Sp.) Photobioreactors Operated in a {{Middle}} and {{Southern European}} Climate},
  author = {Doucha, J. and Lívanský, K.},
  date = {2006-11-22},
  journaltitle = {Journal of Applied Phycology},
  shortjournal = {J Appl Phycol},
  volume = {18},
  number = {6},
  pages = {811--826},
  issn = {0921-8971, 1573-5176},
  doi = {10.1007/s10811-006-9100-4},
  url = {http://link.springer.com/10.1007/s10811-006-9100-4},
  urldate = {2025-03-14},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\RYCETD78\Doucha und Lívanský - 2006 - Productivity, CO2O2 exchange and hydraulics in outdoor open high density microalgal (Chlorella sp.).pdf}
}

@article{eduljiMobileSemiAutonomousRobot,
  title = {A {{Mobile Semi-Autonomous Robot For Soil Sampling}}},
  author = {Edulji, Vistasp and Soman, Sumedh and Pradhan, Atharva and Shah, Jay},
  url = {https://www.authorea.com/users/682676/articles/678127-a-mobile-semi-autonomous-robot-for-soil-sampling},
  urldate = {2025-04-28},
  abstract = {For agrarian economies such as India, the quality of the soil is critical for maximized yield sustainable cultivation. When a large area is utilized, sample testing of soil is essential. The use of a robotic system for sampling is vital for saving tim},
  file = {C:\Users\noiri\Zotero\storage\ITQ5U538\Edulji et al. - A Mobile Semi-Autonomous Robot For Soil Sampling.pdf}
}

@video{engineeringeducatoracademyImageBasedVisualServoing2021,
  entrysubtype = {video},
  title = {Image-{{Based Visual Servoing}} - {{Robot Control Part}}  1},
  editor = {{Engineering Educator Academy}},
  editortype = {director},
  date = {2021-12-19},
  url = {https://www.youtube.com/watch?v=s_0DdxHjrfA},
  urldate = {2024-03-06},
  abstract = {Basics of computer-vision for image-based control of robotic arms are explained in this video, including the pinhole camera model, perspective projection, intrinsic and extrinsic camera parameters, focal axis and focal length, feature points and feature matching, and difference between position-based visual servo control and image-based visual servo control of robotic arms.}
}

@book{fahimiAutonomousRobotsModeling2009,
  title = {Autonomous {{Robots}}: {{Modeling}}, {{Path Planning}}, and {{Control}}},
  shorttitle = {Autonomous {{Robots}}},
  author = {Fahimi, Farbod},
  date = {2009},
  publisher = {Springer US},
  location = {Boston, MA},
  doi = {10.1007/978-0-387-09538-7},
  url = {https://link.springer.com/10.1007/978-0-387-09538-7},
  urldate = {2024-11-16},
  isbn = {978-0-387-09537-0 978-0-387-09538-7},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\844WU3J3\Fahimi - 2009 - Autonomous Robots Modeling, Path Planning, and Control.pdf}
}

@software{farmbotFarmBotOpenSourceCNC,
  title = {{{FarmBot}} | {{Open-Source CNC Farming}}},
  author = {FarmBot},
  url = {https://farm.bot/},
  urldate = {2024-09-26},
  abstract = {Farming and gardening robots for home, educational, and commercial use. Premium Hardware · Worldwide Shipping · Drag and Drop Farm Designer · Step-by-Step Assembly Instructions · Own Your Food},
  file = {C:\Users\noiri\Zotero\storage\SFXPGFX3\farm.bot.html}
}

@software{farmbotGettingStartedGazebo,
  title = {Getting {{Started}} with {{Gazebo}}? — {{Gazebo}} Harmonic Documentation},
  author = {FarmBot},
  url = {https://gazebosim.org/docs/latest/getstarted/},
  urldate = {2024-09-21},
  file = {C:\Users\noiri\Zotero\storage\QV8PSCHA\getstarted.html}
}

@software{flatscher2btDockerforrobotics2025,
  title = {2b-t/Docker-for-Robotics},
  author = {Flatscher, Tobit},
  date = {2025-03-05T07:29:27Z},
  origdate = {2023-02-05T11:31:22Z},
  url = {https://github.com/2b-t/docker-for-robotics},
  urldate = {2025-03-05},
  abstract = {Collection of best practices for working with Docker/Docker-Compose and the Robot Operating System (ROS/ROS 2) in simulation as well as with hardware and real-time requirements},
  keywords = {best-practice,best-practices,docker,docker-compose,dockerfile,graphic-user-interface,gui,guide,guidelines,network,real-time,robot,robot-operating-system,robotics,ros,ros-noetic,ros2,ros2-humble,tutorial,wsl2}
}

@software{formantGlossaryURDF,
  title = {Glossary | {{URDF}}},
  author = {Formant},
  url = {https://formant.io/resources/glossary/urdf/},
  urldate = {2024-11-14},
  abstract = {URDF provides the information a human operator needs to know to understand the physical shape and size of a robot.},
  file = {C:\Users\noiri\Zotero\storage\DJ9QC8T2\urdf.html}
}

@online{FoxgloveDocsFoxglove,
  title = {Foxglove {{Docs}} | {{Foxglove Docs}}},
  url = {https://docs.foxglove.dev/docs},
  urldate = {2025-04-24},
  abstract = {Foxglove is a purpose-built platform that empowers robotics teams to visually debug robots, build reliable autonomy, and scale their development.},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\7KD5FYCV\docs.html}
}

@online{frohlichRos2_control_demosRos2_control_demo_descriptionDiffbot,
  title = {Ros2\_control\_demos/Ros2\_control\_demo\_description/Diffbot/Urdf/Diffbot\_description.Urdf.Xacro at Master · Grahanoi/Ros2\_control\_demos},
  author = {Fröhlich, Christoph},
  url = {https://github.com/grahanoi/ros2_control_demos/blob/master/ros2_control_demo_description/diffbot/urdf/diffbot_description.urdf.xacro},
  urldate = {2024-12-09},
  file = {C:\Users\noiri\Zotero\storage\S6GZITLU\diffbot_description.urdf.html}
}

@software{frohlichRoscontrolsRos2_control_demosThis,
  title = {Ros-Controls/Ros2\_control\_demos: {{This}} Repository Aims at Providing Examples to Illustrate Ros2\_control and Ros2\_controllers},
  author = {Fröhlich, Christoph},
  url = {https://github.com/ros-controls/ros2_control_demos},
  urldate = {2024-12-05}
}

@article{g.rImplementationStereoVision2018,
  title = {Implementation of a {{Stereo}} Vision Based System for Visual Feedback Control of {{Robotic Arm}} for Space Manipulations},
  author = {G.R, Sangeetha and Kumar, Nishank and P.R., Hari and S, Sasikumar},
  date = {2018},
  journaltitle = {Procedia Computer Science},
  shortjournal = {Procedia Computer Science},
  volume = {133},
  pages = {1066--1073},
  issn = {18770509},
  doi = {10.1016/j.procs.2018.07.031},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050918309761},
  urldate = {2025-04-21},
  langid = {english}
}

@online{Gazebo,
  title = {Gazebo},
  url = {https://app.gazebosim.org},
  urldate = {2025-03-03},
  abstract = {Web application for Gazebo},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\WN8EZFNU\models.html}
}

@online{GazebosimRos_gzIntegration,
  title = {Gazebosim/Ros\_gz: {{Integration}} between {{ROS}} (1 and 2) and {{Gazebo}} Simulation},
  url = {https://github.com/gazebosim/ros_gz/tree/ros2},
  urldate = {2024-12-05},
  file = {C:\Users\noiri\Zotero\storage\RHIAAU87\ros2.html}
}

@online{GettingStartedImage,
  title = {Getting {{Started}} with {{Image Preprocessing}} in {{Python}}},
  url = {https://kaggle.com/code/rimmelasghar/getting-started-with-image-preprocessing-in-python},
  urldate = {2024-04-22},
  abstract = {Explore and run machine learning code with Kaggle Notebooks | Using data from Animals-10},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\CF8WE6PC\getting-started-with-image-preprocessing-in-python.html}
}

@online{GettingStartedNav2,
  title = {Getting {{Started}} — {{Nav2}} 1.0.0 Documentation},
  url = {https://docs.nav2.org/getting_started/index.html},
  urldate = {2025-04-09},
  file = {C:\Users\noiri\Zotero\storage\NZ2ATWMH\index.html}
}

@online{GettingStartedOpen3D,
  title = {Getting Started - {{Open3D}} 0.19.0 Documentation},
  url = {https://www.open3d.org/docs/release/getting_started.html},
  urldate = {2025-04-16},
  file = {C:\Users\noiri\Zotero\storage\8N5R6KJQ\getting_started.html}
}

@inproceedings{gilesDevelopmentFacialRecognition2023,
  title = {Development of a {{Facial Recognition Pantograph Drawing Robot}}},
  booktitle = {2023 3rd {{International Conference}} on {{Robotics}}, {{Electrical}} and {{Signal Processing Techniques}} ({{ICREST}})},
  author = {Giles, Daniel and Glenn, Payton and Burnell, Travis and Flynn, Skylar and Noorani, R.},
  date = {2023-01},
  pages = {62--67},
  doi = {10.1109/ICREST57604.2023.10070086},
  url = {https://ieeexplore.ieee.org/document/10070086},
  urldate = {2025-03-03},
  abstract = {This paper describes the design and application of a solar powered drawing robot that implements face detection, image to sketch conversion, and wireless communication between the robot and the main computer. The robot has the capability to detect and capture a picture of a human face, convert the image to a sketch in the form of a series of lines, and physically draw the image on a whiteboard. Additionally, the robot can draw any selected image stored on the main computer, apply or remove shading from the image, move to a specified location away from the sketch to provide a clear area for the user to erase the board, and can draw diagnostic test rectangles to analyze the mechanical performance of the system. These robotic functions were implemented by first determining the angles and geometry of the arms required to locate a given x-y coordinatepoint. This was done using properties of triangles, as well as the known physical limitations of the pantograph's size. A CAD model of the design was created to incorporate both the calculated geometry and purchased parts, such as the servo motors, axles, and bearings. Existing open-source libraries were used to aid in facial recognition and the conversion of images to a sketch of x-y points. By utilizing the known geometry of the robot, the pen was programmed to move to specified points based on the angles of the two servo motors.},
  eventtitle = {2023 3rd {{International Conference}} on {{Robotics}}, {{Electrical}} and {{Signal Processing Techniques}} ({{ICREST}})},
  keywords = {Face recognition,Geometry,Manipulators,Robot kinematics,Signal processing,Solid modeling,Wireless communication},
  file = {C\:\\Users\\noiri\\Zotero\\storage\\7P3W7X7U\\Giles et al. - 2023 - Development of a Facial Recognition Pantograph Drawing Robot.pdf;C\:\\Users\\noiri\\Zotero\\storage\\NVBK9SGZ\\10070086.html}
}

@online{GitHubStudentDeveloper,
  title = {{{GitHub Student Developer Pack}}},
  url = {https://education.github.com/pack},
  urldate = {2025-04-24},
  abstract = {The best developer tools, free for students. Get your GitHub Student Developer Pack now.},
  langid = {english},
  organization = {GitHub Education}
}

@online{Gitsecret,
  title = {Git-Secret},
  url = {https://sobolevn.me/git-secret/},
  urldate = {2025-04-24},
  abstract = {Shell scripts to encrypt your private data inside a git repository.},
  organization = {git-secret},
  file = {C:\Users\noiri\Zotero\storage\2GYYG5A4\git-secret.html}
}

@online{GitSecretsWhy2020,
  title = {Git {{Secrets}}: {{Why Secrets}} inside {{Git}} Are Such a {{Problem}} - {{GitGuardian Blog}}},
  shorttitle = {Git {{Secrets}}},
  date = {2020-09-04T13:48:10},
  url = {https://blog.gitguardian.com/secrets-credentials-api-git/},
  urldate = {2025-04-24},
  abstract = {Despite secrets like API keys, OAuth tokens, certificates and passwords being extremely sensitive, it is common for these to leak into git repositories through source code. This article looks at why this is true and how we can prevent it.},
  langid = {english},
  organization = {GitGuardian Blog - Take Control of Your Secrets Security},
  file = {C:\Users\noiri\Zotero\storage\ZNEL378L\secrets-credentials-api-git.html}
}

@inproceedings{gobeeVisionbasedPillBlister2023,
  title = {Vision-Based {{Pill Blister Package Inspection System}} Using {{CNN}}},
  booktitle = {Proceedings of the 2023 13th {{International Conference}} on {{Biomedical Engineering}} and {{Technology}}},
  author = {Gobee, Suresh and Durairajah, Vickneswari and Prea, Laurent Eddie Sylvester},
  date = {2023-12-19},
  series = {{{ICBET}} '23},
  pages = {93--98},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3620679.3620694},
  url = {https://dl.acm.org/doi/10.1145/3620679.3620694},
  urldate = {2024-03-03},
  abstract = {This research aimed to develop an automated blister package sorting system based on defects detected using image data. The proposed approach aims to reduce the lead time incurred by manual sorting in the production environment and provide a low-cost automated alternative. The image detection model to detect the defects was developed using YOLOv5 pre-trained object detection model from the PyTorch framework. The dataset collected 350 labeled images, including 40\% of those which were augmented and duplicated using Roboflow image processing. The object detection results were leveraged in multiple ways including detecting the presence of blister package in the frame and for pose estimation, among the mainstream defect detection. The image coordinate was converted to a robot arm coordinate to enable effective manipulation. Dobot Magician Robot arm was used to actuate the blister package and was controlled using its native API. The image processing and robot arm manipulation module was integrated using Python. The detection model held a mean average precision(mAP) of 86.1\% for any random rotations of the blister package, and the sorting rate averaged 7.68s per iteration..},
  isbn = {979-8-4007-0743-8},
  keywords = {Deep learning,Machine Vision,Pill inspection,Robot arm integration}
}

@software{graciaarandaGazeboddspluginsSrcDiff_drive,
  title = {Gazebo-Dds-Plugins/Src/Diff\_drive/{{README}}.Md at Master · Rticommunity/Gazebo-Dds-Plugins},
  author = {Gracía Aranda, Fernando},
  url = {https://github.com/rticommunity/gazebo-dds-plugins/blob/master/src/diff_drive/README.md},
  urldate = {2024-12-09},
  abstract = {DDS Gazebo Plugins enable DDS-based robotic systems to leverage Gazebo's simulation capabilities, such as 3D simulation and computer vision. - rticommunity/gazebo-dds-plugins},
  file = {C:\Users\noiri\Zotero\storage\424DYE4K\README.html}
}

@misc{grahamDigitalTwinAutonomous2024,
  title = {Digital {{Twin}} for {{Autonomous Robots}} Using {{ROS2}}},
  author = {Graham, Noirin},
  date = {2024-12-11},
  url = {https://github.com/digital-twin-autonomous-farmbot/report/blob/master/report/main.pdf},
  langid = {english}
}

@article{grimmondUrbanizationGlobalEnvironmental2007,
  title = {Urbanization and {{Global Environmental Change}}: {{Local Effects}} of {{Urban Warming}}},
  shorttitle = {Urbanization and {{Global Environmental Change}}},
  author = {Grimmond, Sue},
  date = {2007},
  journaltitle = {The Geographical Journal},
  volume = {173},
  number = {1},
  eprint = {30113496},
  eprinttype = {jstor},
  pages = {83--88},
  publisher = {[Wiley, Royal Geographical Society (with the Institute of British Geographers)]},
  issn = {0016-7398},
  url = {https://www.jstor.org/stable/30113496},
  urldate = {2025-03-17},
  file = {C:\Users\noiri\Zotero\storage\PCDM9X7E\Grimmond - 2007 - Urbanization and Global Environmental Change Local Effects of Urban Warming.pdf}
}

@article{hobbieNaturebasedApproachesManaging2020,
  title = {Nature-Based Approaches to Managing Climate Change Impacts in Cities},
  author = {Hobbie, Sarah E. and Grimm, Nancy B.},
  date = {2020-01-27},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {375},
  number = {1794},
  pages = {20190124},
  publisher = {Royal Society},
  doi = {10.1098/rstb.2019.0124},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rstb.2019.0124},
  urldate = {2025-03-17},
  abstract = {Managing and adapting to climate change in urban areas will become increasingly important as urban populations grow, especially because unique features of cities amplify climate change impacts. High impervious cover exacerbates impacts of climate warming through urban heat island effects and of heavy rainfall by magnifying runoff and flooding. Concentration of human settlements along rivers and coastal zones increases exposure of people and infrastructure to climate change hazards, often disproportionately affecting those who are least prepared. Nature-based strategies (NBS), which use living organisms, soils and sediments, and/or landscape features to reduce climate change hazards, hold promise as being more flexible, multi-functional and adaptable to an uncertain and non-stationary climate future than traditional approaches. Nevertheless, future research should address the effectiveness of NBS for reducing climate change impacts and whether they can be implemented at scales appropriate to climate change hazards and impacts. Further, there is a need for accurate and comprehensive cost–benefit analyses that consider disservices and co-benefits, relative to grey alternatives, and how costs and benefits are distributed across different communities. NBS are most likely to be effective and fair when they match the scale of the challenge, are implemented with input from diverse voices and are appropriate to specific social, cultural, ecological and technological contexts. This article is part of the theme issue ‘Climate change and ecosystems: threats, opportunities and solutions’.},
  keywords = {cities,climate change adaptation,green infrastructure,nature-based strategies,urban ecosystems},
  file = {C:\Users\noiri\Zotero\storage\RCDHTAS2\Hobbie und Grimm - 2020 - Nature-based approaches to managing climate change impacts in cities.pdf}
}

@online{Home,
  title = {Home},
  url = {https://opencv.org/},
  urldate = {2025-04-21},
  abstract = {OpenCV provides a real-time optimized Computer Vision library, tools, and hardware. It also supports model execution for Machine Learning (ML) and Artificial Intelligence (AI).},
  langid = {american},
  organization = {OpenCV},
  file = {C:\Users\noiri\Zotero\storage\3GUD82K6\opencv.org.html}
}

@online{HostpointShop,
  title = {Hostpoint {{Shop}}},
  url = {https://admin.hostpoint.ch/customer/Shop/Buy?_dflt_vsid_=5a2b09e2-6c0f-4a78-a670-c197488473aa&_shop_vsid_=2afb25f3-6253-4143-b627-e9019ade339c},
  urldate = {2025-04-24}
}

@online{Images,
  title = {Images},
  url = {https://kubernetes.io/docs/concepts/containers/images/},
  urldate = {2025-03-05},
  abstract = {A container image represents binary data that encapsulates an application and all its software dependencies. Container images are executable software bundles that can run standalone and that make very well defined assumptions about their runtime environment. You typically create a container image of your application and push it to a registry before referring to it in a Pod. This page provides an outline of the container image concept. Note:If you are looking for the container images for a Kubernetes release (such as v1.},
  langid = {english},
  organization = {Kubernetes},
  file = {C:\Users\noiri\Zotero\storage\92J9R737\images.html}
}

@misc{indriAMRSystemAutonomous,
  title = {{{AMR}} System for Autonomous Indoor Navigation in Unknown Environments},
  author = {Indri, Marina},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\7JA6HL33\Indri - AMR system for autonomous indoor navigation in unknown environments.pdf}
}

@online{IndustrialRevolution2025,
  title = {Industrial Revolution},
  date = {2025-03-12},
  url = {https://dictionary.cambridge.org/de/worterbuch/englisch/industrial-revolution},
  urldate = {2025-03-17},
  abstract = {1. the period of time during which work began to be done more by machines in…},
  langid = {ngerman},
  file = {C:\Users\noiri\Zotero\storage\C3HW79PV\industrial-revolution.html}
}

@online{InstallingGazeboROS,
  title = {Installing {{Gazebo}} with {{ROS}} — {{Gazebo}} Harmonic Documentation},
  url = {https://gazebosim.org/docs/harmonic/ros_installation/#summary-of-compatible-ros-and-gazebo-combinations},
  urldate = {2024-09-20},
  file = {C:\Users\noiri\Zotero\storage\9ATSTMZZ\ros_installation.html}
}

@online{InstallingGazeboROSa,
  title = {Installing {{Gazebo}} with {{ROS}} — {{Gazebo}} Harmonic Documentation},
  url = {https://gazebosim.org/docs/harmonic/ros_installation/#summary-of-compatible-ros-and-gazebo-combinations},
  urldate = {2024-11-03},
  file = {C:\Users\noiri\Zotero\storage\J67VZ2VA\ros_installation.html}
}

@article{jDeepLearningbasedPrediction2024,
  title = {Deep Learning-Based Prediction of Plant Height and Crown Area of Vegetable Crops Using {{LiDAR}} Point Cloud},
  author = {J, Reji and Nidamanuri, Rama Rao},
  date = {2024-06-28},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {14},
  number = {1},
  pages = {14903},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-024-65322-8},
  url = {https://www.nature.com/articles/s41598-024-65322-8},
  urldate = {2025-04-16},
  abstract = {Remote sensing has been increasingly used in precision agriculture. Buoyed by the developments in the miniaturization of sensors and platforms, contemporary remote sensing offers data at resolutions finer enough to respond to within-farm variations. LiDAR point cloud, offers features amenable to modelling structural parameters of crops. Early prediction of crop growth parameters helps farmers and other stakeholders dynamically manage farming activities. The objective of this work is the development and application of a deep learning framework to predict plant-level crop height and crown area at different growth stages for vegetable crops. LiDAR point clouds were acquired using a terrestrial laser scanner on five dates during the growth cycles of tomato, eggplant and cabbage on the experimental research farms of the University of Agricultural Sciences, Bengaluru, India. We implemented a hybrid deep learning framework combining distinct features of long-term short memory (LSTM) and Gated Recurrent Unit (GRU) for the predictions of plant height and crown area. The predictions are validated with reference ground truth measurements. These predictions were validated against ground truth measurements. The findings demonstrate that plant-level structural parameters can be predicted well ahead of crop growth stages with around 80\% accuracy. Notably, the LSTM and the GRU models exhibited limitations in capturing variations in structural parameters. Conversely, the hybrid model offered significantly improved predictions, particularly for crown area, with error rates for height prediction ranging from 5 to 12\%, with deviations exhibiting a more balanced distribution between overestimation and underestimation This approach effectively captured the inherent temporal growth pattern of the crops, highlighting the potential of deep learning for precision agriculture applications. However, the prediction quality is relatively low at the advanced growth stage, closer to the harvest. In contrast, the prediction quality is stable across the three different crops. The results indicate the presence of a robust relationship between the features of the LiDAR point cloud and the auto-feature map of the deep learning methods adapted for plant-level crop structural characterization. This approach effectively captured the inherent temporal growth pattern of the crops, highlighting the potential of deep learning for precision agriculture applications.},
  langid = {english},
  keywords = {Engineering,Mathematics and computing,Plant sciences},
  file = {C:\Users\noiri\Zotero\storage\YJ895QFT\J und Nidamanuri - 2024 - Deep learning-based prediction of plant height and crown area of vegetable crops using LiDAR point c.pdf}
}

@book{jefferiesRoboticsCognitiveApproaches2008,
  title = {Robotics and Cognitive Approaches to Spatial Mapping},
  editor = {Jefferies, Margaret E. and Yeap, Wai K.},
  date = {2008},
  series = {Springer Tracts in Advanced Robotics},
  number = {v. 38},
  publisher = {Springer},
  location = {Berlin ; New York},
  isbn = {978-3-540-75386-5},
  pagetotal = {328},
  keywords = {Robotics,Space perception},
  annotation = {OCLC: ocn173721135}
}

@inproceedings{jessenSolarPoweredGrass2023,
  title = {Solar {{Powered Grass Cutter}} with {{Obstacle Avoidance Function Using IoT}}},
  booktitle = {2023 {{International Conference}} on {{Intelligent Sensing}} and {{Industrial Automation}}},
  author = {Jessen, Wong and Tahir, Ir Noor Idayu Binti Mohd and Khan, M.K.A. Ahamed},
  date = {2023-12-09},
  pages = {1--7},
  publisher = {ACM},
  location = {Virtual Event China},
  doi = {10.1145/3632314.3632360},
  url = {https://dl.acm.org/doi/10.1145/3632314.3632360},
  urldate = {2025-03-24},
  eventtitle = {{{ISIA}} 2023: 2023 {{International Conference}} on {{Intelligent Sensing}} and {{Industrial Automation}}},
  isbn = {979-8-4007-0940-1},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\2RZYHMKX\Jessen et al. - 2023 - Solar Powered Grass Cutter with Obstacle Avoidance Function Using IoT.pdf}
}

@inproceedings{jiaControlStrategySimulation2024,
  title = {A {{Control Strategy}} and {{Simulation}} for {{Precision Control}} of {{Robot Arms}}},
  booktitle = {Proceedings of the 2024 4th {{International Conference}} on {{Control}} and {{Intelligent Robotics}}},
  author = {Jia, Wenqi and Wang, Jihong},
  date = {2024-06-21},
  pages = {207--211},
  publisher = {ACM},
  location = {Guangzhou China},
  doi = {10.1145/3687488.3687525},
  url = {https://dl.acm.org/doi/10.1145/3687488.3687525},
  urldate = {2025-03-03},
  eventtitle = {{{ICCIR}} 2024: 2024 4th {{International Conference}} on {{Control}} and {{Intelligent Robotics}}},
  isbn = {979-8-4007-0993-7},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\LKDGPNJX\Jia und Wang - 2024 - A Control Strategy and Simulation for Precision Control of Robot Arms.pdf}
}

@online{JoshnewansArticubot_one,
  title = {Joshnewans/Articubot\_one},
  url = {https://github.com/joshnewans/articubot_one},
  urldate = {2024-12-05}
}

@online{JoshnewansMy_botTemplate,
  title = {Joshnewans/My\_bot: {{Template}} for a {{ROS}} Package},
  url = {https://github.com/joshnewans/my_bot},
  urldate = {2024-12-05}
}

@online{JoshnewansSerial_motor_demo,
  title = {Joshnewans/Serial\_motor\_demo},
  url = {https://github.com/joshnewans/serial_motor_demo},
  urldate = {2024-12-05},
  file = {C:\Users\noiri\Zotero\storage\C7L4P694\serial_motor_demo.html}
}

@article{kamRVizToolkitReal2015,
  title = {{{RViz}}: A Toolkit for Real Domain Data Visualization},
  shorttitle = {{{RViz}}},
  author = {Kam, Hyeong Ryeol and Lee, Sung-Ho and Park, Taejung and Kim, Chang-Hun},
  date = {2015-10},
  journaltitle = {Telecommunication Systems},
  shortjournal = {Telecommun Syst},
  volume = {60},
  number = {2},
  pages = {337--345},
  issn = {1018-4864, 1572-9451},
  doi = {10.1007/s11235-015-0034-5},
  url = {http://link.springer.com/10.1007/s11235-015-0034-5},
  urldate = {2024-11-16},
  langid = {english}
}

@software{kistvisionKistvisionPlant_growth_measurement2025,
  title = {Kistvision/{{Plant}}\_growth\_measurement},
  author = {{kistvision}},
  date = {2025-03-12T03:03:32Z},
  origdate = {2022-12-01T02:37:29Z},
  url = {https://github.com/kistvision/Plant_growth_measurement},
  urldate = {2025-04-20}
}

@online{kumarWhatAreRGBD2022,
  title = {What Are {{RGBD}} Cameras? {{Why RGBD}} Cameras Are Preferred in Some Embedded Vision Applications?},
  shorttitle = {What Are {{RGBD}} Cameras?},
  author = {Kumar, Prabu},
  date = {2022-05-19T15:59:33+00:00},
  url = {https://www.e-consystems.com/blog/camera/technology/what-are-rgbd-cameras-why-rgbd-cameras-are-preferred-in-some-embedded-vision-applications/},
  urldate = {2025-04-19},
  abstract = {There has been a surge in demand for depth cameras in embedded vision applications recently. RGBD cameras are a type of depth camera that amplifies the effectiveness of depth-sensing camera systems by enabling object recognition. Learn what an RGBD camera is and how it works.},
  langid = {american},
  organization = {e-con Systems},
  file = {C:\Users\noiri\Zotero\storage\N2MLW9YX\what-are-rgbd-cameras-why-rgbd-cameras-are-preferred-in-some-embedded-vision-applications.html}
}

@article{latschOptimisationHotwaterApplication2014,
  title = {Optimisation of Hot-Water Application Technology for the Control of Broad-Leaved Dock ({{Rumex}} Obtusifolius)},
  author = {Latsch, Roy and Sauter, Joachim},
  date = {2014-12-21},
  journaltitle = {Journal of Agricultural Engineering},
  volume = {45},
  number = {4},
  pages = {137--145},
  issn = {2239-6268},
  doi = {10.4081/jae.2014.239},
  url = {https://www.agroengineering.org/jae/article/view/jae.2014.239},
  urldate = {2024-07-23},
  abstract = {In organic farming, the control of broad-leaved dock (Rumex obtusifolius) via hot-water treatment of the upper root region (hypocotyl) is a new alternative to the current standard control method involving manual digging-out of the roots. This comparative study looks at five different hot-water application techniques. The aim is to optimise the control method in terms of water and energy requirement to obtain a mortality rate of the treated plants of at least 80\%. The studied parameters were the application, the amount of water, the water temperature, the soil moisture content and the soil type. In total, 813 plants of varying size were treated (120-225 plants per treatment). The success of each treatment was rated 12 weeks after it was applied. Based on the results, the preferred treatment in terms of water and energy requirement was a commercially available rotary nozzle. With this nozzle, for example, at 40 vol.-\% soil moisture, 1.6 L of water at a temperature of 90Â°C was necessary for successful Rumex control. The rotary nozzle could be used as a non-contact system, and was therefore the most user-friendly of the application techniques examined.},
  issue = {4},
  langid = {english},
  keywords = {broad-leaved dock,organic farming.,Rumex obtusifolius,thermal treatment,weed control},
  file = {C:\Users\noiri\Zotero\storage\L7C7U679\Latsch und Sauter - 2014 - Optimisation of hot-water application technology f.pdf}
}

@inproceedings{leDigitalTwinApproach2024,
  title = {Digital Twin Approach for Machining with Robotic Manipulator},
  booktitle = {Proceedings of the 2024 {{International Conference}} on {{Advanced Robotics}}, {{Automation Engineering}} and {{Machine Learning}}},
  author = {Le, Van and Mao, Xuanyu and Tran, Minh and Ding, Songlin},
  date = {2024-06-28},
  pages = {12--17},
  publisher = {ACM},
  location = {Hangzhou China},
  doi = {10.1145/3677454.3677457},
  url = {https://dl.acm.org/doi/10.1145/3677454.3677457},
  urldate = {2024-09-18},
  eventtitle = {{{ARAEML}} 2024: 2024 {{International Conference}} on {{Advanced Robotics}}, {{Automation Engineering}} and {{Machine Learning}}},
  isbn = {979-8-4007-1711-6},
  langid = {english}
}

@inproceedings{liuPerformanceValidationYolo2021,
  title = {Performance {{Validation}} of {{Yolo Variants}} for {{Object Detection}}},
  booktitle = {Proceedings of the 2021 {{International Conference}} on {{Bioinformatics}} and {{Intelligent Computing}}},
  author = {Liu, Kaiyue and Tang, Haitong and He, Shuang and Yu, Qin and Xiong, Yulong and Wang, Nizhuan},
  date = {2021-03-21},
  series = {{{BIC}} 2021},
  pages = {239--243},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3448748.3448786},
  url = {https://dl.acm.org/doi/10.1145/3448748.3448786},
  urldate = {2024-03-04},
  abstract = {Object detection is a core part of an intelligent surveillance system and a fundamental algorithm in the field of identity identification, which is of great practical importance. Since the YOLO series algorithms have good results in terms of accuracy and speed, YOLO and each subsequent version have been surpassing. Thus, in this paper, it carries out experiments on three versions of popular YOLO models such as yolov3, yolov4, and yolov5 (yolov5l, yolov5m, yolov5s, yolov5x). The performance of the three versions of YOLO model is analyzed and summarized by training and predicting the public VOC dataset. Results showed that the yolov4 model is higher than the yolov3 model in terms of mAP values, but slightly lower in terms of speed, while the yolov5 series model is better than the yolov3 and yolov4 models both in terms of mAP values and speed.},
  isbn = {978-1-4503-9000-2},
  keywords = {Deep Learning,Object Detection,PASCAL VOC Dataset,YOLO},
  file = {C:\Users\noiri\Zotero\storage\29WYQF8T\Liu et al. - 2021 - Performance Validation of Yolo Variants for Object.pdf}
}

@inproceedings{luoVisionbased3objectPickplace2017,
  title = {Vision-Based 3-{{D}} Object Pick-and-Place Tasks of Industrial Manipulator},
  booktitle = {2017 {{International Automatic Control Conference}} ({{CACS}})},
  author = {Luo, Guor-Yieh and Cheng, Ming-Yang and Chiang, Chia-Ling},
  date = {2017-11},
  pages = {1--7},
  doi = {10.1109/CACS.2017.8284250},
  url = {https://ieeexplore.ieee.org/abstract/document/8284250},
  urldate = {2024-03-03},
  abstract = {When dealing with complicated tasks such as object pick-and-place, it is harder for robotic arms alone to complete them. One of the possible solutions to overcoming the aforementioned difficulties is to introduce machine vision into the robotic arm system. In this paper, the eye-to-hand camera configuration is adopted in the development of the vision-based automatic pick-and-place systems for 3-D objects. The vision-based automatic pick-and-place system developed in this paper consists of three main sections - calibration of machine vision system, object recognition, and transformations of object coordinates. Experimental results indicate that the vision-based automatic pick-and-place system developed in this paper is able to perform an automatic pick-and-place task for 3-D objects.},
  eventtitle = {2017 {{International Automatic Control Conference}} ({{CACS}})},
  keywords = {Calibration,Cameras,Machine vision,Manipulators,Object recognition,Objection recognition,Robot kinematics,Robot vision systems,Stereo vision},
  file = {C\:\\Users\\noiri\\Zotero\\storage\\2DDJPEXL\\Luo et al. - 2017 - Vision-based 3-D object pick-and-place tasks of in.pdf;C\:\\Users\\noiri\\Zotero\\storage\\HMWKDSGC\\8284250.html}
}

@online{macedon971MobileRobotMoving2023,
  type = {Forum post},
  title = {Mobile Robot Moving around Can Not Visualise in {{RVIZ}}, Why So?},
  author = {Macedon971},
  date = {2023-12-29},
  url = {https://stackoverflow.com/q/77730420},
  urldate = {2024-12-05},
  organization = {Stack Overflow}
}

@article{macenskiRobotOperatingSystem2022,
  title = {Robot {{Operating System}} 2: {{Design}}, Architecture, and Uses in the Wild},
  shorttitle = {Robot {{Operating System}} 2},
  author = {Macenski, Steven and Foote, Tully and Gerkey, Brian and Lalancette, Chris and Woodall, William},
  date = {2022-05-25},
  journaltitle = {Science Robotics},
  shortjournal = {Sci. Robot.},
  volume = {7},
  number = {66},
  pages = {eabm6074},
  issn = {2470-9476},
  doi = {10.1126/scirobotics.abm6074},
  url = {https://www.science.org/doi/10.1126/scirobotics.abm6074},
  urldate = {2024-11-14},
  abstract = {The next chapter of the robotics revolution is well underway with the deployment of robots for a broad range of commercial use cases. Even in a myriad of applications and environments, there exists a common vocabulary of components that robots share—the need for a modular, scalable, and reliable architecture; sensing; planning; mobility; and autonomy. The Robot Operating System (ROS) was an integral part of the last chapter, demonstrably expediting robotics research with freely available components and a modular framework. However, ROS 1 was not designed with many necessary production-grade features and algorithms. ROS 2 and its related projects have been redesigned from the ground up to meet the challenges set forth by modern robotic systems in new and exploratory domains at all scales. In this Review, we highlight the philosophical and architectural changes of ROS 2 powering this new chapter in the robotics revolution. We also show through case studies the influence ROS 2 and its adoption has had on accelerating real robot systems to reliable deployment in an assortment of challenging environments.           ,              This Review describes ROS 2’s design, features, and performance with four case studies on land, air, sea, and even space.},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\LQI9DFWT\Macenski et al. - 2022 - Robot Operating System 2 Design, architecture, and uses in the wild.pdf}
}

@article{macenskiSLAMToolboxSLAM2021,
  title = {{{SLAM Toolbox}}: {{SLAM}} for the Dynamic World},
  shorttitle = {{{SLAM Toolbox}}},
  author = {Macenski, Steve and Jambrecic, Ivona},
  date = {2021-05-13},
  journaltitle = {Journal of Open Source Software},
  shortjournal = {JOSS},
  volume = {6},
  number = {61},
  pages = {2783},
  issn = {2475-9066},
  doi = {10.21105/joss.02783},
  url = {https://joss.theoj.org/papers/10.21105/joss.02783},
  urldate = {2025-04-09},
  file = {C:\Users\noiri\Zotero\storage\YN5ILH7D\Macenski und Jambrecic - 2021 - SLAM Toolbox SLAM for the dynamic world.pdf}
}

@video{magicmarksKinematicChainTheory2021,
  entrysubtype = {video},
  title = {Kinematic {{Chain}} | {{Theory}} of {{Machines}}},
  editor = {{Magic Marks}},
  editortype = {director},
  date = {2021-07-20},
  url = {https://www.youtube.com/watch?v=gaj_cuZvHg0},
  urldate = {2025-03-03}
}

@online{ManufactureDeodorantAntiperspirant,
  title = {Manufacture of {{Deodorant}} and {{Antiperspirant}} - {{US}}},
  url = {https://www.silverson.com/us/resource-library/application-reports/manufacture-of-deodorants-and-antiperspirants},
  urldate = {2024-10-29},
  file = {C:\Users\noiri\Zotero\storage\Q74QJIKR\manufacture-of-deodorants-and-antiperspirants.html}
}

@inproceedings{maruyamaExploringPerformanceROS22016,
  title = {Exploring the Performance of {{ROS2}}},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Embedded Software}}},
  author = {Maruyama, Yuya and Kato, Shinpei and Azumi, Takuya},
  date = {2016-10},
  pages = {1--10},
  publisher = {ACM},
  location = {Pittsburgh Pennsylvania},
  doi = {10.1145/2968478.2968502},
  url = {https://dl.acm.org/doi/10.1145/2968478.2968502},
  urldate = {2024-11-18},
  eventtitle = {{{ESWEEK}}'16: {{TWELFTH EMBEDDED SYSTEM WEEK}}},
  isbn = {978-1-4503-4485-2},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\L6HDSX6E\Maruyama et al. - 2016 - Exploring the performance of ROS2.pdf}
}

@video{mechninjaServoClawMechanical2021,
  entrysubtype = {video},
  title = {Servo {{Claw Mechanical}} Arm Robot | {{Mecanum}} Wheel | {{WiFi Based}}},
  editor = {{Mech Ninja}},
  editortype = {director},
  date = {2021-07-27},
  url = {https://www.youtube.com/watch?v=JcaXUgxeRDo},
  urldate = {2025-04-20}
}

@online{meelYOLORYouOnly2021,
  title = {{{YOLOR}} - {{You Only Learn One Representation}} ({{What}}'s New, 2024)},
  author = {Meel, Vidushi},
  date = {2021-08-06T22:51:08+00:00},
  url = {https://viso.ai/deep-learning/yolor/},
  urldate = {2025-04-19},
  abstract = {What you need to know about YOLOR, the latest state-of-the-art object detection model. Performance comparison to YOLO models.},
  langid = {american},
  organization = {viso.ai},
  file = {C:\Users\noiri\Zotero\storage\SQQQ72D6\yolor.html}
}

@online{microsoftWasIstWindowsSubsystem2023,
  title = {Was ist das Windows-Subsystem für Linux?},
  author = {Microsoft},
  date = {2023-12-05},
  url = {https://learn.microsoft.com/de-de/windows/wsl/about},
  urldate = {2024-12-11},
  abstract = {Erfahren Sie mehr über das Windows-Subsystem für Linux einschließlich der verschiedenen Versionen und Einsatzmöglichkeiten. Microsoft liebt Linux.},
  langid = {ngerman},
  file = {C:\Users\noiri\Zotero\storage\HZPLY4JX\about.html}
}

@online{mobilaneDNAInsectScanFuer,
  title = {DNA InsectScan für grüne Fassaden},
  author = {Mobilane},
  url = {https://mobilane.com/de/dna-insect-scan/},
  urldate = {2025-03-10},
  abstract = {DNA InsectScan: Eine neue und einzigartige innovative Methode, um die Wirksamkeit von grünen Fassaden auf die biologische Vielfalt zu messen.},
  langid = {ngerman},
  organization = {Mobilane},
  file = {C:\Users\noiri\Zotero\storage\FW3JFXYW\dna-insect-scan.html}
}

@online{MV12615MaverickStrada,
  title = {\#{{MV12615 Maverick Strada MT}}},
  url = {http://www.hpiracing.com/en/kit/mv12615},
  urldate = {2025-05-04},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\9T7YVRVJ\mv12615.html}
}

@article{naranjo-camposMethodBottleOpening2024,
  title = {Method for {{Bottle Opening}} with a {{Dual-Arm Robot}}},
  author = {Naranjo-Campos, Francisco J. and Victores, Juan G. and Balaguer, Carlos},
  date = {2024-09-23},
  journaltitle = {Biomimetics},
  shortjournal = {Biomimetics},
  volume = {9},
  number = {9},
  pages = {577},
  issn = {2313-7673},
  doi = {10.3390/biomimetics9090577},
  url = {https://www.mdpi.com/2313-7673/9/9/577},
  urldate = {2025-04-28},
  abstract = {This paper introduces a novel approach to robotic assistance in bottle opening using the dual-arm robot TIAGo++. The solution enhances accessibility by addressing the needs of individuals with injuries or disabilities who may require help with common manipulation tasks. The aim of this paper is to propose a method involving vision, manipulation, and learning techniques to effectively address the task of bottle opening. The process begins with the acquisition of bottle and cap positions using an RGB-D camera and computer vision. Subsequently, the robot picks the bottle with one gripper and grips the cap with the other, each by planning safe trajectories. Then, the opening procedure is executed via a position and force control scheme that ensures both grippers follow the unscrewing path defined by the cap thread. Within the control loop, force sensor information is employed to control the vertical axis movements, while gripper rotation control is achieved through a Deep Reinforcement Learning (DRL) algorithm trained to determine the optimal angle increments for rotation. The results demonstrate the successful training of the learning agent. The experiments confirm the effectiveness of the proposed method in bottle opening with the TIAGo++ robot, showcasing the practical viability of the approach.},
  langid = {english}
}

@online{newansDescribingRobotsURDF,
  type = {Tutorial},
  title = {Describing Robots with {{URDF}} | {{Articulated Robotics}}},
  author = {Newans, Josh},
  url = {https://articulatedrobotics.xyz/tutorials/ready-for-ros/urdf},
  urldate = {2024-11-19},
  abstract = {How would you describe a robot? With URDF of course!},
  langid = {english},
  organization = {Articulated Robotics},
  file = {C:\Users\noiri\Zotero\storage\C5BAS53D\urdf.html}
}

@software{newansJoshnewansRos_arduino_bridgeROS,
  title = {Joshnewans/Ros\_arduino\_bridge: {{ROS}} + {{Arduino}} = {{Robot}}},
  author = {Newans, Josh},
  url = {https://github.com/joshnewans/ros_arduino_bridge},
  urldate = {2024-12-05}
}

@online{newansURDFDesignArticulated,
  title = {{{URDF Design}} | {{Articulated Robotics}}},
  author = {Newans, Josh},
  url = {https://articulatedrobotics.xyz/tutorials/mobile-robot/concept-design/concept-urdf},
  urldate = {2024-12-05},
  abstract = {Creating a rough 3D design with URDF},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\PIU49E92\concept-urdf.html}
}

@article{ngocEfficientEvaluationSLAM2023,
  title = {Efficient {{Evaluation}} of {{SLAM Methods}} and {{Integration}} of {{Human Detection}} with {{YOLO Based}} on {{Multiple Optimization}} in {{ROS2}}},
  author = {Ngoc, Hoang Tran and Vinh, Nghi Nguyen and Nguyen, Nguyen Trung and Quach, Luyl-Da},
  date = {2023},
  journaltitle = {International Journal of Advanced Computer Science and Applications},
  shortjournal = {IJACSA},
  volume = {14},
  number = {11},
  issn = {21565570, 2158107X},
  doi = {10.14569/IJACSA.2023.0141130},
  url = {http://thesai.org/Publications/ViewPaper?Volume=14&Issue=11&Code=IJACSA&SerialNo=30},
  urldate = {2025-03-03},
  abstract = {In the realm of robotics, indoor robotics is an increasingly prominent field, and enhancing robot performance stands out as a crucial concern. This research undertakes a comparative analysis of various Simultaneous Localization and Mapping (SLAM) algorithms with the overarching objective of augmenting the navigational capabilities of robots. This is accomplished within an open-source framework known as the Robotic Operating System (ROS2) in conjunction with additional software components such as RVIZ and Gazebo. The central aim of this study is to identify the most efficient SLAM approach by evaluating map accuracy and the time it takes for a robot model to reach its destinations when employing three distinct SLAM algorithms: GMapping, Cartographer SLAM, and SLAM\_toolbox. Furthermore, this study addresses indoor human detection and tracking assignments, in which we evaluate the effectiveness of YOLOv5, YOLOv6, YOLOv7, and YOLOv8 models in conjunction with various optimization algorithms, including SGD, AdamW, and AMSGrad. The study concludes that YOLOv8 with SGD optimization yields the most favorable outcomes for human detection. These proposed systems are rigorously validated through experimentation, utilizing a simulated Gazebo environment within the Robot Operating System 2 (ROS2).},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\KFARPKDI\Ngoc et al. - 2023 - Efficient Evaluation of SLAM Methods and Integration of Human Detection with YOLO Based on Multiple.pdf}
}

@online{ObjectPlantDetection,
  title = {Object {{Plant Detection Object Detection Dataset}} and {{Pre-Trained Model}} by {{Practice}}},
  url = {https://universe.roboflow.com/practice-aw25w/object-plant-detection},
  urldate = {2025-04-16},
  abstract = {850 open source plant images plus a pre-trained Object Plant Detection model and API. Created by Practice},
  langid = {english},
  organization = {Roboflow},
  file = {C:\Users\noiri\Zotero\storage\HIGSUTEJ\object-plant-detection.html}
}

@online{OpenContainerInitiative,
  title = {About the {{Open Container Initiative}} - {{Open Container Initiative}}},
  url = {https://opencontainers.org/about/overview/},
  urldate = {2025-03-12},
  file = {C:\Users\noiri\Zotero\storage\R6QML7XL\overview.html}
}

@software{openroboticsBinaryInstallationUbuntu,
  title = {Binary {{Installation}} on {{Ubuntu}} — {{Gazebo}} Ionic Documentation},
  author = {Open Robotics},
  url = {https://gazebosim.org/docs/latest/install_ubuntu/},
  urldate = {2024-11-03},
  file = {C:\Users\noiri\Zotero\storage\6WAG8TI9\install_ubuntu.html}
}

@software{openroboticsCreatingLaunchFile,
  title = {Creating a Launch File — {{ROS}} 2 {{Documentation}}: {{Rolling}} Documentation},
  author = {Open Robotics},
  url = {https://docs.ros.org/en/rolling/Tutorials/Intermediate/Launch/Creating-Launch-Files.html},
  urldate = {2024-11-19},
  file = {C:\Users\noiri\Zotero\storage\J7VW3X83\Creating-Launch-Files.html}
}

@software{openroboticsDistributionsROS2,
  title = {Distributions — {{ROS}} 2 {{Documentation}}: {{Rolling}} Documentation},
  author = {Open Robotics},
  url = {https://docs.ros.org/en/rolling/Releases.html},
  urldate = {2024-11-09},
  file = {C:\Users\noiri\Zotero\storage\F5KQZ89P\Releases.html}
}

@software{openroboticsDistributionsROS2a,
  title = {Distributions — {{ROS}} 2 {{Documentation}}: {{Rolling}} Documentation},
  author = {Open Robotics},
  url = {https://docs.ros.org/en/rolling/Releases.html},
  urldate = {2024-11-13},
  file = {C:\Users\noiri\Zotero\storage\7F8A2VEL\Releases.html}
}

@software{openroboticsGazebo,
  title = {About -- {{Gazebo}}},
  author = {Open Robotics},
  url = {https://gazebosim.org/about},
  urldate = {2024-11-16},
  file = {C:\Users\noiri\Zotero\storage\YSQZ8DMU\about.html}
}

@software{openroboticsJazzyJaliscoJazzy,
  title = {Jazzy {{Jalisco}} (Jazzy) — {{ROS}} 2 {{Documentation}}: {{Jazzy}} Documentation},
  author = {Open Robotics},
  url = {https://docs.ros.org/en/jazzy/Releases/Release-Jazzy-Jalisco.html},
  urldate = {2024-11-09},
  file = {C:\Users\noiri\Zotero\storage\54MDS7UC\Release-Jazzy-Jalisco.html}
}

@software{openroboticsROSHome,
  title = {{{ROS}}: {{Home}}},
  author = {Open Robotics},
  url = {https://www.ros.org/},
  urldate = {2024-08-14},
  file = {C:\Users\noiri\Zotero\storage\MF7NDGAH\www.ros.org.html}
}

@software{openroboticsRvizROSWiki,
  title = {Rviz - {{ROS Wiki}}},
  author = {Open Robotics},
  url = {https://wiki.ros.org/rviz},
  urldate = {2024-11-13},
  file = {C:\Users\noiri\Zotero\storage\VXMP55JD\rviz.html}
}

@software{openroboticsSpawnURDFGazebo,
  title = {Spawn {{URDF}} — {{Gazebo}} Ionic Documentation},
  author = {Open Robotics},
  url = {https://gazebosim.org/docs/latest/spawn_urdf/},
  urldate = {2024-11-11},
  file = {C:\Users\noiri\Zotero\storage\FI9DDUM5\spawn_urdf.html}
}

@software{openroboticsUnderstandingNodesROS,
  title = {Understanding Nodes — {{ROS}} 2 {{Documentation}}: {{Rolling}} Documentation},
  author = {Open Robotics},
  url = {https://docs.ros.org/en/rolling/Tutorials/Beginner-CLI-Tools/Understanding-ROS2-Nodes/Understanding-ROS2-Nodes.html},
  urldate = {2024-12-10},
  file = {C:\Users\noiri\Zotero\storage\8PM2TWQG\Understanding-ROS2-Nodes.html}
}

@software{openroboticsUrdfROSWiki,
  title = {Urdf - {{ROS Wiki}}},
  author = {Open Robotics},
  url = {https://wiki.ros.org/urdf},
  urldate = {2024-11-14},
  file = {C:\Users\noiri\Zotero\storage\3SCMW53K\urdf.html}
}

@software{openroboticsUrdfXMLJoint,
  title = {Urdf/{{XML}}/Joint - {{ROS Wiki}}},
  author = {Open Robotics},
  url = {https://wiki.ros.org/urdf/XML/joint},
  urldate = {2024-11-19},
  file = {C:\Users\noiri\Zotero\storage\U3Z96DCG\joint.html}
}

@software{openroboticsUsingColconBuild,
  title = {Using Colcon to Build Packages — {{ROS}} 2 {{Documentation}}: {{Rolling}} Documentation},
  author = {Open Robotics},
  url = {https://docs.ros.org/en/rolling/Tutorials/Beginner-Client-Libraries/Colcon-Tutorial.html},
  urldate = {2024-11-19},
  file = {C:\Users\noiri\Zotero\storage\V7JWMRIA\Colcon-Tutorial.html}
}

@software{openRunningROS2,
  title = {Running {{ROS}} 2 Nodes in {{Docker}} [Community-Contributed] — {{ROS}} 2 {{Documentation}}: {{Rolling}} Documentation},
  author = {Open, Robotics},
  url = {https://docs.ros.org/en/rolling/How-To-Guides/Run-2-nodes-in-single-or-separate-docker-containers.html},
  urldate = {2024-11-18},
  file = {C:\Users\noiri\Zotero\storage\T4M9NLW4\Run-2-nodes-in-single-or-separate-docker-containers.html}
}

@online{opensourceroboticsfoundationGazeboTutorialGazebo,
  title = {Gazebo : {{Tutorial}} : {{Gazebo}} Plugins in {{ROS}}},
  author = {Open Source Robotics Foundation},
  url = {https://classic.gazebosim.org/tutorials?tut=ros_gzplugins},
  urldate = {2024-12-09},
  file = {C:\Users\noiri\Zotero\storage\N465W6HJ\tutorials.html}
}

@software{opensourceroboticsfoundationSDFormatHome,
  title = {{{SDFormat Home}}},
  author = {Open Source Robotics Foundation},
  url = {http://sdformat.org/},
  urldate = {2024-09-30},
  file = {C:\Users\noiri\Zotero\storage\2QMHYAQR\sdformat.org.html}
}

@online{Overview,
  title = {Overview},
  url = {https://kubernetes.io/docs/concepts/overview/},
  urldate = {2025-03-25},
  abstract = {Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.},
  langid = {english},
  organization = {Kubernetes},
  file = {C:\Users\noiri\Zotero\storage\BZDHBXWP\overview.html}
}

@online{OverviewArticulatedRobotics,
  title = {Overview | {{Articulated Robotics}}},
  url = {https://articulatedrobotics.xyz/tutorials/},
  urldate = {2024-12-05},
  abstract = {Here are some tutorials!},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\PEF6RLNH\tutorials.html}
}

@misc{pallecchiVisualLocationContext2024,
  title = {Visual Location and Context Recognition of Laboratory Robots},
  author = {Pallecchi, Gianmaria and Gurova, Mariia and Graham, Noirin},
  date = {2024-05-23},
  langid = {english}
}

@inproceedings{phueakthongDevelopmentMobileRobot2021,
  title = {A {{Development}} of {{Mobile Robot Based}} on {{ROS2}} for {{Navigation Application}}},
  booktitle = {2021 {{International Electronics Symposium}} ({{IES}})},
  author = {Phueakthong, Phuwanat and Varagul, Jittima},
  date = {2021-09},
  pages = {517--520},
  doi = {10.1109/IES53407.2021.9593984},
  url = {https://ieeexplore.ieee.org/document/9593984/},
  urldate = {2025-04-21},
  abstract = {This paper proposes an automatic navigation mobile robot using Robot Operating System2 (ROS2) with low-cost embedded hardware. Utilizing Data Distribution Service (DDS) in ROS2 makes the ROS2 more safe and reliable than ROS1. Cartographer and Navigation2 projects in ROS2 are used for Simultaneous Localization and Mapping (SLAM) with 2D LIDAR and navigation, respectively. Micro-ROS which utilizes DDS for eXtremely Resource-Constrained Environments micro-XRCE-DDS is used for communication between main embedded computer and microcontroller replaces ROS serial communication which is less reliable. The experiments prove that the robot can perform mapping and navigation tasks. A robot can generate a global trajectory in a static map to the goal point, can re-plan the local path in the local map area to avoid coming dynamic obstacles during the mission and navigate itself to reach the goal.},
  eventtitle = {2021 {{International Electronics Symposium}} ({{IES}})},
  keywords = {autonomous mobile robot,Hardware,Laser radar,micro-ROS,Microcontrollers,navigation,Navigation,Reliability,ROS2,Simultaneous localization and mapping,SLAM,Trajectory},
  file = {C:\Users\noiri\Zotero\storage\QSM7KX84\Phueakthong und Varagul - 2021 - A Development of Mobile Robot Based on ROS2 for Navigation Application.pdf}
}

@online{PIDControllerROS2_Control,
  title = {{{PID Controller}} — {{ROS2}}\_{{Control}}: {{Rolling Mar}} 2025 Documentation},
  url = {https://control.ros.org/rolling/doc/ros2_controllers/pid_controller/doc/userdoc.html},
  urldate = {2025-03-03},
  file = {C:\Users\noiri\Zotero\storage\2ANSYN4M\userdoc.html}
}

@online{PlantDatasetOverview,
  title = {Plant {{Dataset}} {$>$} {{Overview}}},
  url = {https://universe.roboflow.com/yolov5-9uyoq/plant-nugpm},
  urldate = {2025-04-16},
  abstract = {380 open source aaaa images plus a pre-trained plant model and API. Created by yolov5},
  langid = {english},
  organization = {Roboflow},
  file = {C:\Users\noiri\Zotero\storage\RXIEBY8A\plant-nugpm.html}
}

@online{PrometheusNodeExporter2020,
  title = {Prometheus Node Exporter on {{Raspberry Pi}} - {{How}} to Install},
  date = {2020-01-17T05:21:21+00:00},
  url = {https://linuxhit.com/prometheus-node-exporter-on-raspberry-pi-how-to-install/},
  urldate = {2025-03-27},
  abstract = {Node exporter is a great way to monitor your Raspberry Pi with Prometheus. Follow these steps to get node exporter running on your Raspberry Pi.},
  langid = {american},
  organization = {Linuxhit},
  file = {C:\Users\noiri\Zotero\storage\M2B7JZG7\prometheus-node-exporter-on-raspberry-pi-how-to-install.html}
}

@article{qiuFieldEstimationMaize2022,
  title = {Field Estimation of Maize Plant Height at Jointing Stage Using an {{RGB-D}} Camera},
  author = {Qiu, Ruicheng and Zhang, Man and He, Yong},
  date = {2022-10-01},
  journaltitle = {The Crop Journal},
  shortjournal = {The Crop Journal},
  series = {Crop Phenotyping Studies with Application to Crop Monitoring},
  volume = {10},
  number = {5},
  pages = {1274--1283},
  issn = {2214-5141},
  doi = {10.1016/j.cj.2022.07.010},
  url = {https://www.sciencedirect.com/science/article/pii/S2214514122001805},
  urldate = {2025-03-03},
  abstract = {Plant height can be used for assessing plant vigor and predicting biomass and yield. Manual measurement of plant height is time-consuming and labor-intensive. We describe a method for measuring maize plant height using an RGB-D camera that captures a color image and depth information of plants under field conditions. The color image was first processed to locate its central area using the S component in HSV color space and the Density-Based Spatial Clustering of Applications with Noise algorithm. Testing showed that the central areas of plants could be accurately located. The point cloud data were then clustered and the plant was extracted based on the located central area. The point cloud data were further processed to generate skeletons, whose end points were detected and used to extract the highest points of the central leaves. Finally, the height differences between the ground and the highest points of the central leaves were calculated to determine plant heights. The coefficients of determination for plant heights manually measured and estimated by the proposed approach were all greater than 0.95. The method can effectively extract the plant from overlapping leaves and estimate its plant height. The proposed method may facilitate maize height measurement and monitoring under field conditions.},
  keywords = {Image processing,Kinect,Maize central area,Maize plant height,Point cloud data},
  file = {C:\Users\noiri\Zotero\storage\KWICGDY6\S2214514122001805.html}
}

@online{qualcommRplidarros2,
  title = {Rplidar-Ros2},
  author = {QualcoMM},
  url = {https://docs.qualcomm.com/bundle/publicresource/topics/80-82645-2/rplidar-ros2_3_1_2.html}
}

@inproceedings{ramisettiAutomaticGrassCutting2024,
  title = {Automatic {{Grass Cutting Robot Using Arduino And Ultrasonic Sensor}}},
  booktitle = {2024 11th {{International Conference}} on {{Reliability}}, {{Infocom Technologies}} and {{Optimization}} ({{Trends}} and {{Future Directions}}) ({{ICRITO}})},
  author = {Ramisetti, Vasanthi and Chowdary, P.Sushma and Bankuru, Chandu and Peda, Sudeepthi and Pasala, Mahendra},
  date = {2024-03},
  pages = {1--4},
  issn = {2769-2884},
  doi = {10.1109/ICRITO61523.2024.10522393},
  url = {https://ieeexplore.ieee.org/document/10522393},
  urldate = {2025-02-28},
  abstract = {In previous times, the task of cutting or mowing grass required a significant amount of time and exertion. The introduction of a novel breed of mowing equipment significantly facilitated the task, however it remains a laborious and time-intensive activity that necessitates oversight. Individuals may personally mow the tiny lawns of their residences, however larger lawns require labor assistance. Regardless of the scenario, the task necessitates an investment of time, effort, or finances. An automatic lawn mower is the solution to this problem. It reduces the amount of time, effort, and labor expenses required. The operation of this machine is governed by an Arduino, which receives input from several sensors and regulates the movement and mowing procedures.In this robotic project, we will build an automatic grass cutter robot or a lawn mower robot using Arduino. The robot can cut the excess grass in the garden automatically. If there is an obstacle in the garden, then it will automatically change its direction. It helps to reduce human efforts. The Automatic Grass Cutting Robot is a modern solution to the labor-intensive task of maintaining lawns and grassy areas. This project aims to design and develop a robot capable of autonomously mowing grass within a predefined area using an Arduino microcontroller and an ultrasonic sensor. The robot's primary function is to navigate the designated area, detect obstacles, and efficiently cut grass using a rotary blade.},
  eventtitle = {2024 11th {{International Conference}} on {{Reliability}}, {{Infocom Technologies}} and {{Optimization}} ({{Trends}} and {{Future Directions}}) ({{ICRITO}})},
  keywords = {Acoustics,Arduino,electric components,LawnMoter,Market research,Microcontrollers,Navigation,Reliability,Robot sensing systems,Sensors},
  file = {C:\Users\noiri\Zotero\storage\ZH9SMNXV\10522393.html}
}

@online{RaspberryPiAI,
  title = {Raspberry {{Pi AI Camera Quick-Start Guide}} - {{Tutorial Australia}}},
  url = {https://core-electronics.com.au/guides/raspberry-pi/raspberry-pi-ai-camera-quickstart-guide/},
  urldate = {2025-05-06},
  abstract = {In this guide, we will be getting the Raspberry Pi AI camera up and running as quickly as possible, as well as taking a look at how to get started with it in your own projects. The Raspberry Pi AI camera is a unique and interesting piece of hardware. It's a standard-sized camera module that not only houses a camera but also a Sony IMX500 AI accelerator chip. This chip is a piece of processing hardware that is optimised to run computer vision tasks such as object recognition, pose estimation, and segmentation. Because this chip is an external piece of processing hardware, the computer vision model is uploaded to the board and all the processing-intensive tasks are run on it instead of the Pi - freeing it up to do other tasks. As it is also a chip designed to run computer vision, it can do so far more efficiently and effectively than even the fastest Pi 5! It has never been easier to get computer vision in your project so let's get right into it.Contents: What You Will Need Hardware Assembly Installing PI OS Software Installation Object Detection and Pose Estimation Demos Implementing in Your Own Projects What You Will NeedTo follow along with this guide you will need a: Raspberry PI - While this camera will work on most Raspberry Pis, we used the Pi 5. AI Camera Module Power Supply Micro SD Card Monitor and Micro-HDMI to HDMI Cable Mouse and Keyboard Hardware AssemblyIn your Ai Camera box you will find 2 ribbon cables. One of them will have both ends the same size, and another will have a smaller end on one. This cable with the smaller end on is the cable required for the Pi 5, Zero and Zero 2W. To insert the cable, lift the tab on the bottom of the camera, insert the cable, and then push the tab back down. Ensure that the cable is sitting nice and square in the connector.~ Repeat the process on the Pi. If you are using a Pi 5, plug it into camera connector one as shown in the image.Installing Pi OSFirst things first, we need to install Pi OS onto the micro SD card. Using the Raspberry Pi Imager, select Raspberry PI 5 as the Device, Raspberry Pi OS (64-bit) as the Operating system, and your microSD card as the storage device. NOTE: INSTALLING PI OS ONTO THE MICROSD CARD WILL WIPE ALL DATA ON IT. This process may take a few minutes to download the OS and install it. Once the process has finished, insert it into the Pi and boot it up. Your Pi will run through a first-time installation and just ensure that you connect it to the internet. Software InstallationNow we will install the required drivers and software for the camera to operate. Open up a terminal window and type in the following command to double-check check your Pi is updated: sudo apt update \&\& sudo apt full-upgrade You may need to hit "y" and "enter" to confirm. Then install the camera package with: sudo apt install imx500-all This process may take 5-10 minutes depending on your model of Pi. Once that has finished, restart your Raspberry Pi. If you want to be a power user, you can restart the Pi from the terminal with: rebootObject Detection and Pose Estimation DemosNow we are ready to run our first set of demos, starting with object recognition. To do so open a terminal window again and paste in the following command: rpicam-hello -t 0s --post-process-file /usr/share/rpi-camera-assets/imx500\_mobilenet\_ssd.json --viewfinder-width 1920 --viewfinder-height 1080 --framerate 30 The object recognition model will be uploaded to the camera, and after a few seconds, you will see a preview window of your camera with boxes being drawn around an object as well as a label and confidence rating.Implementing it in your own ProjectsWhile it's easy to get Raspberry Pi's demo going, there are a few hurdles in getting it implemented in your own projects. We have however tried our best to simplify this process by adapting some of their code into a library that allows for easy use of the camera in Python. Please note for the time being this is an object detection only example. Before using this library you will need to install OpenCV. Open a new terminal window and enter: sudo apt install python3-opencv python3-munkres Once that has been installed, download and unzip the following zip file to your desktop. ai-camera-library-demo.zip In there you will find 2 files, "ai\_camera.py" is the library itself, and demo.py is a demo Python script using the library. Open it in a Python editor like Thonny which comes pre-installed on your Pi. This code starts the camera, creates a new preview window and then uses the detection results to do something if it detects a certain object. Here is a breakdown of how it works: The code begins by importing the library ai\_camera.py. This file needs to be in the same folder as the demo script for it to work. It also imports the time library.{$<$}pre id="python" class="prettyprint" style="width: auto; overflow: auto; max-height: 400px;"{$>$} from ai\_camera import IMX500Detector import time Then we create an object called camera with the library. This is a variable that will be called to use camera functions. For example we immidiately use camera.start to start the AI camera up. We can also specify whether we want the preview window to appear here or not. At this point the AI camera is running and analysing footage, but we aren't doing anything with it.{$<$}pre id="python" class="prettyprint" style="width: auto; overflow: auto; max-height: 400px;"{$>$} camera = IMX500Detector() \# Start the detector with preview window camera.start(show\_preview=True) Then we enter into our main while true loop. Here we start by getting the detections and labels from the camera. These simply grab what ever the camera has seen in the last frame and could be called at anytime as long as the camera is running.{$<$}pre id="python" class="prettyprint" style="width: auto; overflow: auto; max-height: 400px;"{$>$} \# Main loop while True:     \# Get the latest detections     detections = camera.get\_detections()          \# Get the labels for reference     labels = camera.get\_labels() Then we run through each item detected in the frame and assign the label to it, as well as get the confidence rating of each one. This ranges from 0 (no confidence) to 1.0 (maximum confidence). We also check each detected object and if they are a "person" and have a confidence rating above 0.4, we will print out a sentence.~ Here is the main part of the script that you may want to modify as it has the logic to perform an action if a specific object is detected. Change the object to something that can be detected, and set the confidence as needed. You can also change the action that is performed, here we simply print to the shell, but you could turn on an LED, motor or alarm, send an email, or however you want to apply it in your project!{$<$}pre id="python" class="prettyprint" style="width: auto; overflow: auto; max-height: 400px;"{$>$}     \# Process each detection     for detection in detections:         label = labels[int(detection.category)]         confidence = detection.conf         \# Example: Print when a person is detected with high confidence         if label == "person" and confidence {$>$} 0.4:             print(f"Person detected with \{confidence:.2f\} confidence!") And finally at the end of the loop we sleep for 0.1 seconds. Change this to however frequently you wish to run this entire loop. You will be limited by the FPS of the camera which is about 30.{$<$}pre id="python" class="prettyprint" style="width: auto; overflow: auto; max-height: 400px;"{$>$}         \# Example: Print when a person is detected with high confidence         if label == "person" and confidence {$>$} 0.4:             print(f"Person detected with \{confidence:.2f\} confidence!") And with this framework you should now be able to utilised the process of the AI camera in your own projects!},
  langid = {english},
  organization = {Core Electronics},
  file = {C:\Users\noiri\Zotero\storage\F36F9L6I\raspberry-pi-ai-camera-quickstart-guide.html}
}

@article{redmonYOLOv3IncrementalImprovement,
  title = {{{YOLOv3}}: {{An Incremental Improvement}}},
  author = {Redmon, Joseph and Farhadi, Ali},
  abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that’s pretty swell. It’s a little bigger than last time but more accurate. It’s still fast though, don’t worry. At 320 × 320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 AP50 in 51 ms on a Titan X, compared to 57.5 AP50 in 198 ms by RetinaNet, similar performance but 3.8× faster. As always, all the code is online at https://pjreddie.com/yolo/.},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\NGSIBB5F\Redmon und Farhadi - YOLOv3 An Incremental Improvement.pdf}
}

@inproceedings{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  date = {2016-06},
  pages = {779--788},
  publisher = {IEEE},
  location = {Las Vegas, NV, USA},
  doi = {10.1109/CVPR.2016.91},
  url = {http://ieeexplore.ieee.org/document/7780460/},
  urldate = {2024-03-04},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  keywords = {Methods},
  file = {C:\Users\noiri\Zotero\storage\53C78TH7\Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf}
}

@article{riveraUnmannedGroundVehicle2019,
  title = {Unmanned {{Ground Vehicle Modelling}} in {{Gazebo}}/{{ROS-Based Environments}}},
  author = {Rivera, Zandra B. and De Simone, Marco C. and Guida, Domenico},
  date = {2019-06},
  journaltitle = {Machines},
  volume = {7},
  number = {2},
  pages = {42},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2075-1702},
  doi = {10.3390/machines7020042},
  url = {https://www.mdpi.com/2075-1702/7/2/42},
  urldate = {2024-12-09},
  abstract = {The fusion of different technologies is the base of the fourth industrial revolution. Companies are encouraged to integrate new tools in their production processes in order to improve working conditions and increase productivity and production quality. The integration between information, communication technologies and industrial automation can create highly flexible production models for products and services that can be customized through real-time interactions between consumer, production and machinery throughout the production process. The future of production, therefore, depends on increasingly intelligent machinery through the use of digital systems. The key elements for future integrated devices are intelligent systems and machines, based on human–machine interaction and information sharing. To do so, the implementation of shared languages that allow different systems to dialogue in a simple way is necessary. In this perspective, the use of advanced prototyping tools like Open-Source programming systems, the development of more detailed multibody models through the use of CAD software and the use of self-learning techniques will allow for developing a new class of machines capable of revolutionizing our companies. The purpose of this paper is to present a waypoint navigation activity of a custom Wheeled Mobile Robot (WMR) in an available simulated 3D indoor environment by using the Gazebo simulator. Gazebo was developed in 2002 at the University of Southern California. The idea was to create a high-fidelity simulator that gave the possibility to simulate robots in outdoor environments under various conditions. In particular, we wanted to test the high-performance physics Open Dynamics Engine (ODE) and the sensors feature present in Gazebo for prototype development activities. This choice was made for the possibility of emulating not only the system under analysis, but also the world in which the robot will operate. Furthermore, the integration tools available with Solidworks and Matlab-Simulink, well known commercial platforms of modelling and robotics control respectively, are also explored.},
  issue = {2},
  langid = {english},
  keywords = {gazebo,Matlab,multibody dynamics,robotics,wheeled mobile robot},
  file = {C:\Users\noiri\Zotero\storage\JK4BAJF6\Rivera et al. - 2019 - Unmanned Ground Vehicle Modelling in GazeboROS-Based Environments.pdf}
}

@online{roboticsunveiledROS2Part122024,
  title = {{{ROS2 Part}} 12 - {{ROS2 Digital Twin}}},
  author = {RoboticsUnveiled},
  date = {2024-05-21T04:35:21+02:00},
  url = {https://www.roboticsunveiled.com/ros2-ros2-digital-twin/},
  urldate = {2024-08-15},
  abstract = {In this post we present an overview on the concept of ROS2 Digital Twins.},
  langid = {american},
  organization = {ROS2 Part 12 – ROS2 Digital Twin},
  file = {C:\Users\noiri\Zotero\storage\X6QWJYAU\ros2-ros2-digital-twin.html}
}

@online{ROS2Foxglove,
  title = {{{ROS}} 2 | {{Foxglove Docs}}},
  url = {https://docs.foxglove.dev/docs/connecting-to-data/frameworks/ros2},
  urldate = {2025-03-31},
  abstract = {Load local and remote MCAP files containing ROS 2 data, or connect directly to a live ROS 2 stack.},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\CPKY7CXE\ros2.html}
}

@software{Rplidar_ros,
  title = {Rplidar\_ros},
  url = {https://index.ros.org/p/rplidar_ros/?utm_source=chatgpt.com}
}

@online{RunningROS2,
  title = {Running {{ROS}} 2 on {{Multiple Machines}} | {{Husarion}}},
  url = {https://husarion.com/tutorials/ros2-tutorials/6-robot-network/},
  urldate = {2025-03-03},
  abstract = {Learn to configure ROS for efficient information exchange among multiple robots on both local networks and the Internet. Gain the power to command all your robots from a single location with ease.},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\A3CRZMU9\6-robot-network.html}
}

@online{schwabFourthIndustrialRevolution2016,
  title = {The {{Fourth Industrial Revolution}}: What It Means and How to Respond},
  shorttitle = {The {{Fourth Industrial Revolution}}},
  author = {Schwab, Klaus},
  date = {2016-01-14},
  url = {https://www.weforum.org/stories/2016/01/the-fourth-industrial-revolution-what-it-means-and-how-to-respond/},
  urldate = {2025-03-17},
  abstract = {The Fourth Industrial Revolution: what it means and how to respond, by Klaus Schwab},
  langid = {english},
  organization = {World Economic Forum},
  file = {C:\Users\noiri\Zotero\storage\GBITYJMH\the-fourth-industrial-revolution-what-it-means-and-how-to-respond.html}
}

@online{SettingNodeExporter,
  title = {Setting {{Up Node Exporter}} - {{Techdox Docs}}},
  url = {https://docs.techdox.nz/node-exporter/},
  urldate = {2025-03-27},
  abstract = {The Node Exporter is a project that is maintained through the Prometheus project.},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\744L5Q2C\node-exporter.html}
}

@online{SettingOdometryNav2,
  title = {Setting {{Up Odometry}} — {{Nav2}} 1.0.0 Documentation},
  url = {https://docs.nav2.org/setup_guides/odom/setup_odom.html},
  urldate = {2024-12-06},
  file = {C:\Users\noiri\Zotero\storage\YC4X894P\setup_odom.html}
}

@online{SlamtecRplidar_rosRos2,
  title = {Slamtec/Rplidar\_ros at Ros2},
  url = {https://github.com/Slamtec/rplidar_ros},
  urldate = {2025-05-02},
  abstract = {Contribute to Slamtec/rplidar\_ros development by creating an account on GitHub.},
  langid = {english},
  organization = {GitHub}
}

@article{songComprehensiveReview3D2025,
  title = {Comprehensive Review on {{3D}} Point Cloud Segmentation in Plants},
  author = {Song, Hongli and Wen, Weiliang and Wu, Sheng and Guo, Xinyu},
  date = {2025-06-01},
  journaltitle = {Artificial Intelligence in Agriculture},
  shortjournal = {Artificial Intelligence in Agriculture},
  volume = {15},
  number = {2},
  pages = {296--315},
  issn = {2589-7217},
  doi = {10.1016/j.aiia.2025.01.006},
  url = {https://www.sciencedirect.com/science/article/pii/S2589721725000066},
  urldate = {2025-04-16},
  abstract = {Segmentation of three-dimensional (3D) point clouds is fundamental in comprehending unstructured structural and morphological data. It plays a critical role in research related to plant phenomics, 3D plant modeling, and functional-structural plant modeling. Although technologies for plant point cloud segmentation (PPCS) have advanced rapidly, there has been a lack of a systematic overview of the development process. This paper presents an overview of the progress made in 3D point cloud segmentation research in plants. It starts by discussing the methods used to acquire point clouds in plants, and analyzes the impact of point cloud resolution and quality on the segmentation task. It then introduces multi-scale point cloud segmentation in plants. The paper summarizes and analyzes traditional methods for PPCS, including the global and local features. This paper discusses the progress of machine learning-based segmentation on plant point clouds through supervised, unsupervised, and integrated approaches. It also summarizes the datasets that for PPCS using deep learning-oriented methods and explains the advantages and disadvantages of deep learning-based methods for projection-based, voxel-based, and point-based approaches respectively. Finally, the development of PPCS is discussed and prospected. Deep learning methods are predicted to become dominant in the field of PPCS, and 3D point cloud segmentation would develop towards more automated with higher resolution and precision.},
  keywords = {Deep learning,Multi-scale,Plant,Point cloud,Segmentation,Three-dimensional},
  file = {C:\Users\noiri\Zotero\storage\8ZC5EEZW\S2589721725000066.html}
}

@article{songMechanicalEfficiencyKinematics1988,
  title = {The Mechanical Efficiency and Kinematics of Pantograph-Type Manipulators},
  author = {Song, Shin-Min and Lee, Jong-Kil},
  date = {1988-03-01},
  journaltitle = {KSME Journal},
  shortjournal = {KSME Journal},
  volume = {2},
  number = {1},
  pages = {69--78},
  issn = {1738-494X},
  doi = {10.1007/BF02944079},
  url = {https://doi.org/10.1007/BF02944079},
  urldate = {2025-03-03},
  abstract = {Pantograph mechanism has been well known for its motion feature of decoupled kinematics. Planar pantograph mechanism has been extensively used in machinery since the seventeenth century. Recently, three dimensional pantographs have been used in walking machine leg and manipulator designs. This is because, the pantograph mechanism possesses the following advantages decoupled kinematics, higher energy efficiency, good rigidity, less link inertia and compact drive systems. In this paper, the mechanical efficiency of the kinematics of pantograph type manipulators are studied. The mechanical efficiency of pantograph mechanisms and conventional open-chain and closed-chain type manipulators are studied and evaluated using the concept of modified geometric work. The kinematics of six-d.o.f., pantograph type manipulators are studied and special mechanisms which simplify the kinematics are introduced. The computational complexity of both Cartesian and cylindrical type pantograph manipulators are evaluated and compared with a PUMA type manipulator.},
  langid = {english},
  keywords = {Inverse Position Analysis,Mechanical Efficiency,Pantograph Mechanism},
  file = {C:\Users\noiri\Zotero\storage\43L7GNV4\Song und Lee - 1988 - The mechanical efficiency and kinematics of pantograph-type manipulators.pdf}
}

@article{staczekDigitalTwinApproach2021,
  title = {A {{Digital Twin Approach}} for the {{Improvement}} of an {{Autonomous Mobile Robots}} ({{AMR}}’s) {{Operating Environment}}—{{A Case Study}}},
  author = {Stączek, Paweł and Pizoń, Jakub and Danilczuk, Wojciech and Gola, Arkadiusz},
  date = {2021-11-25},
  journaltitle = {Sensors},
  shortjournal = {Sensors},
  volume = {21},
  number = {23},
  pages = {7830},
  issn = {1424-8220},
  doi = {10.3390/s21237830},
  url = {https://www.mdpi.com/1424-8220/21/23/7830},
  urldate = {2024-11-16},
  abstract = {The contemporary market creates a demand for continuous improvement of production, service, and management processes. Increasingly advanced IT technologies help designers to meet this demand, as they allow them to abandon classic design and design-testing methods in favor of techniques that do not require the use of real-life systems and thus significantly reduce the costs and time of implementing new solutions. This is particularly important when re-engineering production and logistics processes in existing production companies, where physical testing is often infeasible as it would require suspension of production for the testing period. In this article, we showed how the Digital Twin technology can be used to test the operating environment of an autonomous mobile robot (AMR). In particular, the concept of the Digital Twin was used to assess the correctness of the design assumptions adopted for the early phase of the implementation of an AMR vehicle in a company’s production hall. This was done by testing and improving the case of a selected intralogistics task in a potentially “problematic” part of the shop floor with narrow communication routes. Three test scenarios were analyzed. The results confirmed that the use of digital twins could accelerate the implementation of automated intralogistics systems and reduce its costs.},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\44QC3UP2\Stączek et al. - 2021 - A Digital Twin Approach for the Improvement of an Autonomous Mobile Robots (AMR’s) Operating Environ.pdf}
}

@online{SwarmMode0800,
  title = {Swarm Mode},
  year = {07:31:40 -0800 -0800},
  url = {https://docs.docker.com/engine/swarm/},
  urldate = {2025-03-23},
  abstract = {Docker Engine Swarm mode overview},
  langid = {english},
  organization = {Docker Documentation}
}

@online{SwarmModeKey0200,
  title = {Swarm Mode Key Concepts},
  year = {15:26:47 +0200 +0200},
  url = {https://docs.docker.com/engine/swarm/key-concepts/},
  urldate = {2025-03-23},
  abstract = {Introducing key concepts for Docker Engine swarm mode},
  langid = {english},
  organization = {Docker Documentation},
  file = {C:\Users\noiri\Zotero\storage\TVKHQCXS\key-concepts.html}
}

@inproceedings{takayaSimulationEnvironmentMobile2016,
  title = {Simulation Environment for Mobile Robots Testing Using {{ROS}} and {{Gazebo}}},
  booktitle = {2016 20th {{International Conference}} on {{System Theory}}, {{Control}} and {{Computing}} ({{ICSTCC}})},
  author = {Takaya, Kenta and Asai, Toshinori and Kroumov, Valeri and Smarandache, Florentin},
  date = {2016-10},
  pages = {96--101},
  publisher = {IEEE},
  location = {Sinaia},
  doi = {10.1109/ICSTCC.2016.7790647},
  url = {https://ieeexplore.ieee.org/document/7790647/},
  urldate = {2024-11-16},
  eventtitle = {2016 20th {{International Conference}} on {{System Theory}}, {{Control}} and {{Computing}} ({{ICSTCC}})},
  isbn = {978-1-5090-2720-0}
}

@video{techdoxEffortlessServerMonitoring2023,
  entrysubtype = {video},
  title = {Effortless {{Server Monitoring}}: {{Install Grafana}}, {{Prometheus}} \& {{Node Exporter}} with {{Docker}}!},
  shorttitle = {Effortless {{Server Monitoring}}},
  editor = {{Techdox}},
  editortype = {director},
  date = {2023-12-30},
  url = {https://www.youtube.com/watch?v=yrscZ-kGc_Y},
  urldate = {2025-03-27}
}

@online{TraefikProxyDocumentation,
  title = {Traefik {{Proxy Documentation}} - {{Traefik}}},
  url = {https://doc.traefik.io/traefik/},
  urldate = {2025-04-05},
  file = {C:\Users\noiri\Zotero\storage\2WSW7ZEA\traefik.html}
}

@online{Triangulation_YDLIDAR|FocusLidarSensor,
  title = {Triangulation\_{{YDLIDAR}}|{{Focus}} on Lidar Sensor Solutions},
  url = {https://www.ydlidar.com/products/Triangulation.html},
  urldate = {2025-04-11},
  file = {C:\Users\noiri\Zotero\storage\2HVLAWQ7\Triangulation.html}
}

@online{ultralyticsRaspberryPi,
  title = {Raspberry {{Pi}}},
  author = {Ultralytics},
  url = {https://docs.ultralytics.com/guides/raspberry-pi},
  urldate = {2025-04-16},
  abstract = {Learn how to deploy Ultralytics YOLO11 on Raspberry Pi with our comprehensive guide. Get performance benchmarks, setup instructions, and best practices.},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\UHY77PPD\raspberry-pi.html}
}

@online{UnmetDependenciesInstalling,
  title = {Unmet Dependencies Installing Ros-Jazzy-Desktop on {{Ubuntu}} 24.04 {{LTS}} · {{Issue}} \#1621 · Ros2/Ros2},
  url = {https://github.com/ros2/ros2/issues/1621},
  urldate = {2025-04-12},
  abstract = {Bug report Ubuntu 24.04.1 LTS, kernel 6.9.1 Steps to reproduce issue Follow steps from https://docs.ros.org/en/jazzy/Installation/Ubuntu-Install-Debs.html Expected behavior Install OK Actual behavi...},
  langid = {english},
  organization = {GitHub},
  file = {C:\Users\noiri\Zotero\storage\2GYQWVN8\1621.html}
}

@online{UpdatedGuideDocker2023,
  title = {An {{Updated Guide}} to {{Docker}} and {{ROS}} 2 – {{Robotic Sea Bass}}},
  date = {2023-07-09},
  url = {https://roboticseabass.com/2023/07/09/updated-guide-docker-and-ros2/},
  urldate = {2025-03-05},
  langid = {american},
  file = {C:\Users\noiri\Zotero\storage\ZCD2NB4C\updated-guide-docker-and-ros2.html}
}

@software{UrdfXMLROS,
  title = {Urdf/{{XML}} - {{ROS Wiki}}},
  url = {https://wiki.ros.org/urdf/XML},
  urldate = {2024-11-19},
  file = {C:\Users\noiri\Zotero\storage\TWLJPF5H\XML.html}
}

@online{UseROS2,
  title = {Use {{ROS}} 2 to Interact with {{Gazebo}} — {{Gazebo}} Ionic Documentation},
  url = {https://gazebosim.org/docs/latest/ros2_integration/},
  urldate = {2024-12-05}
}

@online{VaultHashiCorpDeveloper,
  title = {Vault | {{HashiCorp Developer}}},
  url = {https://developer.hashicorp.com/vault},
  urldate = {2025-04-24},
  abstract = {Explore Vault product documentation, tutorials, and examples.},
  langid = {english},
  organization = {Vault | HashiCorp Developer},
  file = {C:\Users\noiri\Zotero\storage\L26LUHP4\vault.html}
}

@online{voasDraftConsiderationsDigital2021,
  title = {({{Draft}}) {{Considerations}} for {{Digital Twins Standards}}},
  author = {Voas, Jeff},
  date = {2021-04-16},
  doi = {10.6028/NIST.IR.8356-draft},
  url = {https://nvlpubs.nist.gov/nistpubs/ir/2021/NIST.IR.8356-draft.pdf},
  urldate = {2024-11-16},
  pubstate = {prepublished},
  file = {C:\Users\noiri\Zotero\storage\NJE52RHU\Voas - 2021 - (Draft) Considerations for Digital Twins Standards.pdf}
}

@misc{wahdeINTRODUCTIONAUTONOMOUSROBOTS2016,
  title = {{{INTRODUCTION TO AUTONOMOUS ROBOTS}}},
  author = {Wahde, Mattias},
  date = {2016},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\Q5J7WWVZ\Wahde - INTRODUCTION TO AUTONOMOUS ROBOTS.pdf}
}

@software{wangSllidar_ros2,
  title = {Sllidar\_ros2},
  author = {Wang, DeYou},
  url = {https://github.com/Slamtec/sllidar_ros2}
}

@online{wangYouOnlyLearn2021,
  title = {You {{Only Learn One Representation}}: {{Unified Network}} for {{Multiple Tasks}}},
  shorttitle = {You {{Only Learn One Representation}}},
  author = {Wang, Chien-Yao and Yeh, I.-Hau and Liao, Hong-Yuan Mark},
  date = {2021-05-10},
  eprint = {2105.04206},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2105.04206},
  url = {http://arxiv.org/abs/2105.04206},
  urldate = {2025-04-19},
  abstract = {People ``understand'' the world via vision, hearing, tactile, and also the past experience. Human experience can be learned through normal learning (we call it explicit knowledge), or subconsciously (we call it implicit knowledge). These experiences learned through normal learning or subconsciously will be encoded and stored in the brain. Using these abundant experience as a huge database, human beings can effectively process data, even they were unseen beforehand. In this paper, we propose a unified network to encode implicit knowledge and explicit knowledge together, just like the human brain can learn knowledge from normal learning as well as subconsciousness learning. The unified network can generate a unified representation to simultaneously serve various tasks. We can perform kernel space alignment, prediction refinement, and multi-task learning in a convolutional neural network. The results demonstrate that when implicit knowledge is introduced into the neural network, it benefits the performance of all tasks. We further analyze the implicit representation learnt from the proposed unified network, and it shows great capability on catching the physical meaning of different tasks. The source code of this work is at : https://github.com/WongKinYiu/yolor.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\noiri\\Zotero\\storage\\MS9K7JSF\\Wang et al. - 2021 - You Only Learn One Representation Unified Network for Multiple Tasks.pdf;C\:\\Users\\noiri\\Zotero\\storage\\V3LABBEF\\2105.html}
}

@online{WebhooksDocumentationGitHub,
  title = {Webhooks Documentation - {{GitHub Enterprise Server}} 3.14 {{Docs}}},
  url = {https://docs-internal.github.com/en/enterprise-server@3.14/webhooks},
  urldate = {2025-03-28},
  abstract = {Webhooks can let your integrations take an action in response to events that occur on GitHub.},
  langid = {english},
  organization = {GitHub Docs},
  file = {C:\Users\noiri\Zotero\storage\KYIHA8QN\webhooks.html}
}

@inproceedings{weissPlantSpeciesClassification2010,
  title = {Plant {{Species Classification Using}} a {{3D LIDAR Sensor}} and {{Machine Learning}}},
  booktitle = {2010 {{Ninth International Conference}} on {{Machine Learning}} and {{Applications}}},
  author = {Weiss, Ulrich and Biber, Peter and Laible, Stefan and Bohlmann, Karsten and Zell, Andreas},
  date = {2010-12},
  pages = {339--345},
  doi = {10.1109/ICMLA.2010.57},
  url = {https://ieeexplore.ieee.org/abstract/document/5708854},
  urldate = {2025-04-16},
  abstract = {In the domain of agricultural robotics, one major application is crop scouting, e.g., for the task of weed control. For this task a key enabler is a robust detection and classification of the plant and species. Automatically distinguishing between plant species is a challenging task, because some species look very similar. It is also difficult to translate the symbolic high level description of the appearances and the differences between the plants used by humans, into a formal, computer understandable form. Also it is not possible to reliably detect structures, like leaves and branches in 3D data provided by our sensor. One approach to solve this problem is to learn how to classify the species by using a set of example plants and machine learning methods. In this paper we are introducing a method for distinguishing plant species using a 3D LIDAR sensor and supervised learning. For that we have developed a set of size and rotation invariant features and evaluated experimentally which are the most descriptive ones. Besides these features we have also compared different learning methods using the toolbox Weka. It turned out that the best methods for our application are simple logistic regression functions, support vector machines and neural networks. In our experiments we used six different plant species, typically available at common nurseries, and about 20 examples of each species. In the laboratory we were able to identify over 98\% of these plants correctly.},
  eventtitle = {2010 {{Ninth International Conference}} on {{Machine Learning}} and {{Applications}}},
  keywords = {3D laser sensor,agricultural robotics,Agriculture,Artificial neural networks,Histograms,Lasers,plant classification,Robot sensing systems,supervised learning,Three dimensional displays},
  file = {C:\Users\noiri\Zotero\storage\ZM7Z85SQ\Weiss et al. - 2010 - Plant Species Classification Using a 3D LIDAR Sensor and Machine Learning.pdf}
}

@online{WelcomeOxfordNanopore,
  title = {Welcome to {{Oxford Nanopore Technologies}}},
  url = {https://nanoporetech.com/},
  urldate = {2025-03-10},
  abstract = {Discover a new generation of molecular sensing technology which offers short to ultra-long native DNA and RNA reads.},
  langid = {british},
  organization = {Oxford Nanopore Technologies},
  file = {C:\Users\noiri\Zotero\storage\RN5G3G9H\nanoporetech.com.html}
}

@online{WhatCloudComputing2023,
  title = {What {{Is Cloud Computing}}? | {{IBM}}},
  shorttitle = {What {{Is Cloud Computing}}?},
  date = {2023-04-25T00:00:00.000},
  url = {https://www.ibm.com/topics/cloud-computing},
  urldate = {2024-08-16},
  abstract = {Cloud computing enables customers to use infrastructure and applications by way of the internet, without installing and maintaining them on premises.},
  langid = {american}
}

@online{WhatContainer0100,
  title = {What Is a Container?},
  year = {11:25:13 +0100 +0100},
  url = {https://docs.docker.com/get-started/docker-concepts/the-basics/what-is-a-container/},
  urldate = {2025-03-25},
  abstract = {What is a container? This concept page will teach you about containers and provide a quick hands-on where you will run your first container.},
  langid = {english},
  organization = {Docker Documentation},
  file = {C:\Users\noiri\Zotero\storage\M3KUIACG\what-is-a-container.html}
}

@online{WhatEdgeComputing,
  title = {What {{Is Edge Computing}}? {{Everything You Need}} to {{Know}}},
  shorttitle = {What {{Is Edge Computing}}?},
  url = {https://www.techtarget.com/searchdatacenter/definition/edge-computing},
  urldate = {2024-08-16},
  abstract = {Learn about edge computing, how it works and the importance of its role in the growth of 5G. Discover why edge computing matters, including benefits and use cases.},
  langid = {english},
  organization = {Data Center},
  file = {C:\Users\noiri\Zotero\storage\HV28A3NB\edge-computing.html}
}

@online{WhyUseCompose0100,
  title = {Why Use {{Compose}}?},
  year = {16:22:09 +0100 +0100},
  url = {https://docs.docker.com/compose/intro/features-uses/},
  urldate = {2025-03-23},
  abstract = {Key benefits and use cases of Docker Compose},
  langid = {english},
  organization = {Docker Documentation},
  file = {C:\Users\noiri\Zotero\storage\RU76BGK6\features-uses.html}
}

@software{YDLIDARYdlidar_ros2_driver2025,
  title = {{{YDLIDAR}}/Ydlidar\_ros2\_driver},
  date = {2025-04-09T00:45:36Z},
  origdate = {2020-06-17T07:19:26Z},
  url = {https://github.com/YDLIDAR/ydlidar_ros2_driver},
  urldate = {2025-04-11},
  abstract = {ydlidar driver package under ros2},
  organization = {Shenzhen Yuedeng Technology Co.,Ltd.}
}

@software{YDLIDARYdlidar_ros2_driver2025a,
  title = {{{YDLIDAR}}/Ydlidar\_ros2\_driver},
  date = {2025-04-09T00:45:36Z},
  origdate = {2020-06-17T07:19:26Z},
  url = {https://github.com/YDLIDAR/ydlidar_ros2_driver},
  urldate = {2025-04-15},
  abstract = {ydlidar driver package under ros2},
  organization = {Shenzhen Yuedeng Technology Co.,Ltd.}
}

@online{YOLO11RealTimeCrop,
  title = {{{YOLO11}} for {{Real-Time Crop Health Monitoring}} | {{Ultralytics}}},
  url = {https://www.ultralytics.com/blog/real-time-crop-health-monitoring-with-ultralytics-yolo11},
  urldate = {2025-04-16},
  abstract = {Join us as we take a closer look at how Ultralytics YOLO11 reimagines real-time crop health monitoring through plant disease detection and weed detection.},
  langid = {english},
  file = {C:\Users\noiri\Zotero\storage\5UMZLZQF\real-time-crop-health-monitoring-with-ultralytics-yolo11.html}
}
